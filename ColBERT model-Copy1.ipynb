{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# CPU\n",
      "\n",
      "\n",
      "\n",
      "# RAM\n",
      "\n",
      "# GPU\n",
      "\n",
      "# OS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from ast import literal_eval\n",
    "\n",
    "def run(command):\n",
    "    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)\n",
    "    out, err = process.communicate()\n",
    "    print(out.decode('utf-8').strip())\n",
    "\n",
    "print('# CPU')\n",
    "run('cat /proc/cpuinfo | egrep -m 1 \"^model name\"')\n",
    "run('cat /proc/cpuinfo | egrep -m 1 \"^cpu MHz\"')\n",
    "run('cat /proc/cpuinfo | egrep -m 1 \"^cpu cores\"')\n",
    "\n",
    "print('# RAM')\n",
    "run('cat /proc/meminfo | egrep \"^MemTotal\"')\n",
    "\n",
    "print('# GPU')\n",
    "run('lspci | grep VGA')\n",
    "\n",
    "print('# OS')\n",
    "run('uname -a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "# import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "# import bert_tokenization as tokenization\n",
    "import tensorflow.keras.backend as K\n",
    "import os\n",
    "from scipy.stats import spearmanr\n",
    "from math import floor, ceil\n",
    "from transformers import *\n",
    "\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re    #for regex\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "MODEL_TYPE = 'bert-base-uncased'\n",
    "MAX_SIZE = 200\n",
    "BATCH_SIZE = 200\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read data and tokenizer\n",
    "\n",
    "Read tokenizer and data, as well as defining the maximum sequence length that will be used for the input to Bert (maximum is usually 512 tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAS_ANS = False\n",
    "training_sample_count = 1000 # 4000\n",
    "training_epochs = 1 # 3\n",
    "test_count = 1000\n",
    "running_folds = 1 # 2\n",
    "MAX_SEQUENCE_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/input/200k-short-texts-for-humor-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/input/cbert-after-preprocessing-by-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>humor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joe biden rules out 2020 bid: 'guys, i'm not r...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Watch: darvish gave hitter whiplash with slow ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What do you call a turtle without its shell? d...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  humor\n",
       "0  Joe biden rules out 2020 bid: 'guys, i'm not r...  False\n",
       "1  Watch: darvish gave hitter whiplash with slow ...  False\n",
       "2  What do you call a turtle without its shell? d...   True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>humor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What kind of cat should you take into the  des...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Remember when people used to have to be in sha...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pizza is always good. - everyone we'll see abo...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  humor\n",
       "0  What kind of cat should you take into the  des...   True\n",
       "1  Remember when people used to have to be in sha...   True\n",
       "2  Pizza is always good. - everyone we'll see abo...   True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "df_train = pd.read_csv('train.csv')\n",
    "display(df_train.head(3))\n",
    "df_train = df_train[:training_sample_count*5]\n",
    "\n",
    "df_test = pd.read_csv('dev.csv')\n",
    "display(df_test.head(3))\n",
    "df_test = df_test[:test_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 5000 1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>humor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joe biden rules out 2020 bid: 'guys, i'm not r...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Watch: darvish gave hitter whiplash with slow ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What do you call a turtle without its shell? d...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5 reasons the 2016 election feels so personal</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pasco police shot mexican migrant from behind,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  humor\n",
       "0  Joe biden rules out 2020 bid: 'guys, i'm not r...  False\n",
       "1  Watch: darvish gave hitter whiplash with slow ...  False\n",
       "2  What do you call a turtle without its shell? d...   True\n",
       "3      5 reasons the 2016 election feels so personal  False\n",
       "4  Pasco police shot mexican migrant from behind,...  False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What kind of cat should you take into the  des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Remember when people used to have to be in sha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pizza is always good. - everyone we'll see abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What's 6 inches long hard, bent, and in my pan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Black teen's response to violence in his commu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  What kind of cat should you take into the  des...\n",
       "1  Remember when people used to have to be in sha...\n",
       "2  Pizza is always good. - everyone we'll see abo...\n",
       "3  What's 6 inches long hard, bent, and in my pan...\n",
       "4  Black teen's response to violence in his commu..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df_y = df_test.copy()\n",
    "del df_test['humor']\n",
    "\n",
    "df_sub = test_df_y.copy()\n",
    "\n",
    "print(len(df),len(df_train),len(df_test))\n",
    "display(df_train.head())\n",
    "display(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'humor']\n"
     ]
    }
   ],
   "source": [
    "print(list(df_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input categories:\n",
      "\t ['text']\n",
      "\n",
      "output TARGET_COUNT:\n",
      "\t 1\n",
      "\n",
      "output categories:\n",
      "\t ['humor']\n"
     ]
    }
   ],
   "source": [
    "output_categories = list(df_train.columns[[1]])\n",
    "input_categories = list(df_train.columns[[0]])\n",
    "\n",
    "if HAS_ANS:\n",
    "    output_categories = list(df_train.columns[11:])\n",
    "    input_categories = list(df_train.columns[[1,2,5]])\n",
    "    \n",
    "\n",
    "TARGET_COUNT = len(output_categories)\n",
    "\n",
    "print('\\ninput categories:\\n\\t', input_categories)\n",
    "print('\\noutput TARGET_COUNT:\\n\\t', TARGET_COUNT)\n",
    "print('\\noutput categories:\\n\\t', output_categories)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### 2. Preprocessing functions\n",
    "\n",
    "These are some functions that will be used to preprocess the raw text data into useable Bert inputs.<br>\n",
    "\n",
    "*update 4:* credits to [Minh](https://www.kaggle.com/dathudeptrai) for this implementation. If I'm not mistaken, it could be used directly with other Huggingface transformers too! Note that due to the 2 x 512 input, it will require significantly more memory when finetuning BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n",
    "    \n",
    "    def return_id(str1, str2, truncation_strategy, length):\n",
    "\n",
    "        inputs = tokenizer.encode_plus(str1, str2,\n",
    "            add_special_tokens=True,\n",
    "            max_length=length,\n",
    "            truncation_strategy=truncation_strategy)\n",
    "        \n",
    "        input_ids =  inputs[\"input_ids\"]\n",
    "        input_masks = [1] * len(input_ids)\n",
    "        input_segments = inputs[\"token_type_ids\"]\n",
    "        padding_length = length - len(input_ids)\n",
    "        padding_id = tokenizer.pad_token_id\n",
    "        input_ids = input_ids + ([padding_id] * padding_length)\n",
    "        input_masks = input_masks + ([0] * padding_length)\n",
    "        input_segments = input_segments + ([0] * padding_length)\n",
    "        \n",
    "        return [input_ids, input_masks, input_segments]\n",
    "    \n",
    "    input_ids_q, input_masks_q, input_segments_q = return_id(\n",
    "        title, None, 'longest_first', max_sequence_length)\n",
    "    \n",
    "    input_ids_a, input_masks_a, input_segments_a = return_id(\n",
    "        '', None, 'longest_first', max_sequence_length)\n",
    "        \n",
    "    return [input_ids_q, input_masks_q, input_segments_q,\n",
    "            input_ids_a, input_masks_a, input_segments_a]\n",
    "\n",
    "def compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n",
    "    input_ids_q, input_masks_q, input_segments_q = [], [], []\n",
    "    input_ids_a, input_masks_a, input_segments_a = [], [], []\n",
    "    for _, instance in tqdm(df[columns].iterrows()):\n",
    "        t, q, a = instance.text, instance.text, instance.text\n",
    "\n",
    "        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n",
    "        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n",
    "        \n",
    "        input_ids_q.append(ids_q)\n",
    "        input_masks_q.append(masks_q)\n",
    "        input_segments_q.append(segments_q)\n",
    "        input_ids_a.append(ids_a)\n",
    "        input_masks_a.append(masks_a)\n",
    "        input_segments_a.append(segments_a)\n",
    "        \n",
    "    return [np.asarray(input_ids_q, dtype=np.int32), \n",
    "            np.asarray(input_masks_q, dtype=np.int32), \n",
    "            np.asarray(input_segments_q, dtype=np.int32),\n",
    "            np.asarray(input_ids_a, dtype=np.int32), \n",
    "            np.asarray(input_masks_a, dtype=np.int32), \n",
    "            np.asarray(input_segments_a, dtype=np.int32)]\n",
    "\n",
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd77e0e38b1f4fb2a9e696e7609f28e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c385f721994e2387a068a3beed1f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = compute_output_arrays(df_train, output_categories)\n",
    "inputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "test_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create model\n",
    "\n",
    "~~`compute_spearmanr()`~~ `mean_squared_error` is used to compute the competition metric for the validation set\n",
    "<br><br>\n",
    "`create_model()` contains the actual architecture that will be used to finetune BERT to our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spearmanr_ignore_nan(trues, preds):\n",
    "    rhos = []\n",
    "    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n",
    "        rhos.append(spearmanr(tcol, pcol).correlation)\n",
    "    return np.nanmean(rhos)\n",
    "\n",
    "def create_model():\n",
    "    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    config = BertConfig() # print(config) to see settings\n",
    "    config.output_hidden_states = False # Set to True to obtain hidden states\n",
    "    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n",
    "    \n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-uncased', config=config)\n",
    "    \n",
    "    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n",
    "    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
    "    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n",
    "    \n",
    "    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n",
    "    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n",
    "    \n",
    "#     x = tf.keras.layers.Concatenate()([q, q])\n",
    "    \n",
    "    x = tf.keras.layers.Dropout(0.2)(q)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(TARGET_COUNT, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, ], outputs=x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Obtain inputs and targets, as well as the indices of the train/validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['humor']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training, validation and testing\n",
    "\n",
    "Loops over the folds in gkf and trains each fold for 3 epochs --- with a learning rate of 3e-5 and batch_size of 6. A simple binary crossentropy is used as the objective-/loss-function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== \n",
      "mean_absolute_error  : 0.09999999999999999\n",
      "mean_squared_error  : 0.009999999999999998\n",
      "r2 score  : 0.96\n",
      "================== \n",
      "balanced_accuracy_score  : 0.5\n",
      "average_precision_score  : 0.5\n",
      "balanced_accuracy_score  : 0.5\n",
      "accuracy_score  : 0.5\n",
      "f1_score  : 0.6666666666666666\n",
      "[[0 1]\n",
      " [0 1]]\n",
      "Acc 0.5 Prec 0.5 Rec 1.0 F1 0.6666666666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation Metrics\n",
    "import sklearn\n",
    "def print_evaluation_metrics(y_true, y_pred, label='', is_regression=True, label2=''):\n",
    "    print('==================', label2)\n",
    "    ### For regression\n",
    "    if is_regression:\n",
    "        print('mean_absolute_error',label,':', sklearn.metrics.mean_absolute_error(y_true, y_pred))\n",
    "        print('mean_squared_error',label,':', sklearn.metrics.mean_squared_error(y_true, y_pred))\n",
    "        print('r2 score',label,':', sklearn.metrics.r2_score(y_true, y_pred))\n",
    "        #     print('max_error',label,':', sklearn.metrics.max_error(y_true, y_pred))\n",
    "        return sklearn.metrics.mean_squared_error(y_true, y_pred)\n",
    "    else:\n",
    "        ### FOR Classification\n",
    "        print('balanced_accuracy_score',label,':', sklearn.metrics.balanced_accuracy_score(y_true, y_pred))\n",
    "        print('average_precision_score',label,':', sklearn.metrics.average_precision_score(y_true, y_pred))\n",
    "        print('balanced_accuracy_score',label,':', sklearn.metrics.balanced_accuracy_score(y_true, y_pred))\n",
    "        print('accuracy_score',label,':', sklearn.metrics.accuracy_score(y_true, y_pred))\n",
    "        print('f1_score',label,':', sklearn.metrics.f1_score(y_true, y_pred))\n",
    "        \n",
    "        matrix = sklearn.metrics.confusion_matrix(y_true, y_pred)\n",
    "        print(matrix)\n",
    "        TP,TN,FP,FN = matrix[1][1],matrix[0][0],matrix[0][1],matrix[1][0]\n",
    "        Accuracy = (TP+TN)/(TP+FP+FN+TN)\n",
    "        Precision = TP/(TP+FP)\n",
    "        Recall = TP/(TP+FN)\n",
    "        F1 = 2*(Recall * Precision) / (Recall + Precision)\n",
    "        print('Acc', Accuracy, 'Prec', Precision, 'Rec', Recall, 'F1',F1)\n",
    "        return sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "\n",
    "print_evaluation_metrics([1,0], [0.9,0.1], '', True)\n",
    "print_evaluation_metrics([1,0], [1,1], '', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function selection\n",
    "Regression problem between 0 and 1, so binary_crossentropy and mean_absolute_error seem good.\n",
    "\n",
    "Here are the explanations: https://www.dlology.com/blog/how-to-choose-last-layer-activation-and-loss-function/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "LR= 1e-05\n",
      "(6, 1000, 100) (1000, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "167/167 [==============================] - 622s 4s/step - loss: 0.2688\n",
      "================== \n",
      "mean_absolute_error on #1 : 0.089022234\n",
      "mean_squared_error on #1 : 0.04109716\n",
      "r2 score on #1 : 0.8355692675469366\n",
      "new acc >>  0.04109716\n",
      " \n"
     ]
    }
   ],
   "source": [
    "min_acc = 1000000\n",
    "min_test = []\n",
    "valid_preds = []\n",
    "test_preds = []\n",
    "best_model = False\n",
    "for LR in np.arange(1e-5, 2e-5, 3e-5).tolist():\n",
    "    print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    print('LR=', LR)\n",
    "    gkf = GroupKFold(n_splits=5).split(X=df_train.text, groups=df_train.text)\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
    "        if fold not in range(running_folds):\n",
    "            continue\n",
    "        train_inputs = [(inputs[i][train_idx])[:training_sample_count] for i in range(len(inputs))]\n",
    "        train_outputs = (outputs[train_idx])[:training_sample_count]\n",
    "\n",
    "        valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n",
    "        valid_outputs = outputs[valid_idx]\n",
    "\n",
    "        print(np.array(train_inputs).shape, np.array(train_outputs).shape)\n",
    "#         print(train_idx[:10], valid_idx[:10])\n",
    "\n",
    "        K.clear_session()\n",
    "        model = create_model()\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "        model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "        for xx in range(1):\n",
    "            model.fit(train_inputs, train_outputs, epochs=training_epochs, batch_size=6)\n",
    "            # model.save_weights(f'bert-{fold}.h5')\n",
    "            valid_preds.append(model.predict(valid_inputs))\n",
    "            #rho_val = compute_spearmanr_ignore_nan(valid_outputs, valid_preds[-1])\n",
    "            #print('validation score = ', rho_val)\n",
    "            acc = print_evaluation_metrics(np.array(valid_outputs), np.array(valid_preds[-1]), 'on #'+str(xx+1))\n",
    "            if acc < min_acc:\n",
    "                print('new acc >> ', acc)\n",
    "                min_acc = acc\n",
    "                best_model = model\n",
    "#                 min_test = model.predict(test_inputs)\n",
    "#                 test_preds.append(min_test)\n",
    "            print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best acc >>  0.04109716\n"
     ]
    }
   ],
   "source": [
    "print('best acc >> ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1) (1000, 1)\n",
      "================== \n",
      "mean_absolute_error  : 0.062727354\n",
      "mean_squared_error  : 0.04007975\n",
      "r2 score  : 0.8396399568198802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04007975"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(valid_outputs.shape, valid_preds[-1].shape)\n",
    "print_evaluation_metrics(np.array(valid_outputs), np.array(valid_preds[-1]), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "min_test = best_model.predict(test_inputs)\n",
    "\n",
    "## use min_test\n",
    "# min_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('df_test.csv')\n",
    "test_df_y.to_csv('test_df_y.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== \n",
      "mean_absolute_error  : 0.076710254\n",
      "mean_squared_error  : 0.05096099\n",
      "r2 score  : 0.7959470943635406\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>humor</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What kind of cat should you take into the  des...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.997979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Remember when people used to have to be in sha...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.969810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pizza is always good. - everyone we'll see abo...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.978942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What's 6 inches long hard, bent, and in my pan...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.996369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Black teen's response to violence in his commu...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.002182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  humor      pred\n",
       "0  What kind of cat should you take into the  des...   True  0.997979\n",
       "1  Remember when people used to have to be in sha...   True  0.969810\n",
       "2  Pizza is always good. - everyone we'll see abo...   True  0.978942\n",
       "3  What's 6 inches long hard, bent, and in my pan...   True  0.996369\n",
       "4  Black teen's response to violence in his commu...  False  0.002182"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub = test_df_y.copy()\n",
    "# df_sub['pred'] = np.average(test_preds, axis=0) # for weighted average set weights=[...]\n",
    "df_sub['pred'] = min_test\n",
    "\n",
    "\n",
    "print_evaluation_metrics(df_sub['humor'], df_sub['pred'], '', True)\n",
    "\n",
    "\n",
    "df_sub.to_csv('sub1.csv', index=False)\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== SPLIT on 0.1\n",
      "balanced_accuracy_score  : 0.9042459478505991\n",
      "average_precision_score  : 0.8498221195532606\n",
      "balanced_accuracy_score  : 0.9042459478505991\n",
      "accuracy_score  : 0.907\n",
      "f1_score  : 0.916591928251121\n",
      "[[396  88]\n",
      " [  5 511]]\n",
      "Acc 0.907 Prec 0.8530884808013356 Rec 0.9903100775193798 F1 0.916591928251121\n",
      "================== SPLIT on 0.2\n",
      "balanced_accuracy_score  : 0.916834838875008\n",
      "average_precision_score  : 0.868798676433308\n",
      "balanced_accuracy_score  : 0.916834838875008\n",
      "accuracy_score  : 0.919\n",
      "f1_score  : 0.9261622607110301\n",
      "[[411  73]\n",
      " [  8 508]]\n",
      "Acc 0.919 Prec 0.8743545611015491 Rec 0.9844961240310077 F1 0.9261622607110301\n",
      "================== SPLIT on 0.30000000000000004\n",
      "balanced_accuracy_score  : 0.925163367288103\n",
      "average_precision_score  : 0.8814290310756323\n",
      "balanced_accuracy_score  : 0.925163367288103\n",
      "accuracy_score  : 0.927\n",
      "f1_score  : 0.9328426862925483\n",
      "[[420  64]\n",
      " [  9 507]]\n",
      "Acc 0.927 Prec 0.8879159369527145 Rec 0.9825581395348837 F1 0.9328426862925483\n",
      "================== SPLIT on 0.4\n",
      "balanced_accuracy_score  : 0.9274216798001154\n",
      "average_precision_score  : 0.8863855590895947\n",
      "balanced_accuracy_score  : 0.9274216798001154\n",
      "accuracy_score  : 0.929\n",
      "f1_score  : 0.9341983317886933\n",
      "[[425  59]\n",
      " [ 12 504]]\n",
      "Acc 0.929 Prec 0.8952042628774423 Rec 0.9767441860465116 F1 0.9341983317886933\n",
      "================== SPLIT on 0.5\n",
      "balanced_accuracy_score  : 0.9315539112050739\n",
      "average_precision_score  : 0.8926423430544577\n",
      "balanced_accuracy_score  : 0.9315539112050739\n",
      "accuracy_score  : 0.933\n",
      "f1_score  : 0.9376744186046511\n",
      "[[429  55]\n",
      " [ 12 504]]\n",
      "Acc 0.933 Prec 0.9016100178890877 Rec 0.9767441860465116 F1 0.9376744186046511\n",
      "================== SPLIT on 0.6\n",
      "balanced_accuracy_score  : 0.9338122237170863\n",
      "average_precision_score  : 0.8978240408559489\n",
      "balanced_accuracy_score  : 0.9338122237170863\n",
      "accuracy_score  : 0.935\n",
      "f1_score  : 0.9390815370196812\n",
      "[[434  50]\n",
      " [ 15 501]]\n",
      "Acc 0.935 Prec 0.9092558983666061 Rec 0.9709302325581395 F1 0.9390815370196812\n",
      "================== SPLIT on 0.7000000000000001\n",
      "balanced_accuracy_score  : 0.9433019411877763\n",
      "average_precision_score  : 0.9146938562998959\n",
      "balanced_accuracy_score  : 0.9433019411877763\n",
      "accuracy_score  : 0.944\n",
      "f1_score  : 0.9467680608365019\n",
      "[[446  38]\n",
      " [ 18 498]]\n",
      "Acc 0.944 Prec 0.9291044776119403 Rec 0.9651162790697675 F1 0.9467680608365019\n",
      "================== SPLIT on 0.8\n",
      "balanced_accuracy_score  : 0.9414920878980075\n",
      "average_precision_score  : 0.9143358198040077\n",
      "balanced_accuracy_score  : 0.9414920878980075\n",
      "accuracy_score  : 0.942\n",
      "f1_score  : 0.9445506692160612\n",
      "[[448  36]\n",
      " [ 22 494]]\n",
      "Acc 0.942 Prec 0.9320754716981132 Rec 0.9573643410852714 F1 0.9445506692160612\n",
      "================== SPLIT on 0.9\n",
      "balanced_accuracy_score  : 0.9393538983919534\n",
      "average_precision_score  : 0.9210021884198684\n",
      "balanced_accuracy_score  : 0.9393538983919534\n",
      "accuracy_score  : 0.939\n",
      "f1_score  : 0.9401373895976447\n",
      "[[460  24]\n",
      " [ 37 479]]\n",
      "Acc 0.939 Prec 0.952286282306163 Rec 0.9282945736434108 F1 0.9401373895976447\n"
     ]
    }
   ],
   "source": [
    "for split in np.arange(0.1, 0.99, 0.1).tolist():\n",
    "    df_sub['pred_bi'] = (df_sub['pred'] > split)\n",
    "\n",
    "    print_evaluation_metrics(df_sub['humor'], df_sub['pred_bi'], '', False, 'SPLIT on '+str(split))\n",
    "\n",
    "    df_sub.to_csv('sub3.csv', index=False)\n",
    "    df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
