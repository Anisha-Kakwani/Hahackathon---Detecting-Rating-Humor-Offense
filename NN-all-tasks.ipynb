{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.3.1\n",
      "Eager mode:  True\n",
      "Hub version:  0.9.0\n",
      "GPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mError_analysis\u001b[m\u001b[m/                Untitled.ipynb\r\n",
      "Is-Humor_NN.ipynb              Untitled1.ipynb\r\n",
      "Naive_Bayes_week5.ipynb        abc.txt\r\n",
      "Naive_Bayes_week5_nisha.ipynb  eval_data.csv\r\n",
      "Predict_is_humor.ipynb         new_dff-2.xlsx\r\n",
      "TASk_1a.csv                    new_dff.csv\r\n",
      "TASk_1a_LR.csv                 public_dev.csv\r\n",
      "TASk_1a_LR.zip                 train.csv\r\n",
      "TF_notebook.ipynb              week_4_NN.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                               text  is_humor  \\\n",
      "0   1  TENNESSEE: We're the best state. Nobody even c...         1   \n",
      "1   2  A man inserted an advertisement in the classif...         1   \n",
      "2   3  How many men does it take to open a can of bee...         1   \n",
      "3   4  Told my mom I hit 1200 Twitter followers. She ...         1   \n",
      "4   5  Roses are dead. Love is fake. Weddings are bas...         1   \n",
      "\n",
      "   humor_rating  humor_controversy  offense_rating  \n",
      "0          2.42                1.0             0.2  \n",
      "1          2.50                1.0             1.1  \n",
      "2          1.95                0.0             2.4  \n",
      "3          2.11                1.0             0.0  \n",
      "4          2.78                0.0             0.1  \n",
      "    id                                               text  is_humor  \\\n",
      "0    1  TENNESSEE: We're the best state. Nobody even c...         1   \n",
      "1    2  A man inserted an advertisement in the classif...         1   \n",
      "3    4  Told my mom I hit 1200 Twitter followers. She ...         1   \n",
      "7    8  ME: I'm such an original. Truly one of a kind....         1   \n",
      "12  13  Stop calling 9-1-1 because you've run out of t...         1   \n",
      "13  14  When you march the streets shouting with peopl...         1   \n",
      "17  18  You can make any sentence creepier by adding \"...         1   \n",
      "19  20  me: if ant-man shrinks by making the space bet...         1   \n",
      "25  26  My wife left me because she said I made a meal...         1   \n",
      "26  27  How do the Chinese select their baby names? Th...         1   \n",
      "30  31  I asked a pretty, young, homeless woman if I c...         1   \n",
      "37  38  Anybody that eats Tide pods is an idiot. They ...         1   \n",
      "45  46  What do you call bad breath that sneaks up on ...         1   \n",
      "46  47  What does a gay rooster say? \"Anycockledoooooo...         1   \n",
      "47  48  How can you tell when an Italian car has a fla...         1   \n",
      "49  50  I never finish anything. I have a black belt i...         1   \n",
      "51  52  Two meth heads start a relationship, is that c...         1   \n",
      "62  63  Hellen Keller walks into a bar. And then a tab...         1   \n",
      "63  64  The year is 2125, we are touring the american ...         1   \n",
      "68  69  I only believe 12.5% of the Bible. Which means...         1   \n",
      "\n",
      "    humor_rating  humor_controversy  offense_rating  \n",
      "0           2.42                1.0            0.20  \n",
      "1           2.50                1.0            1.10  \n",
      "3           2.11                1.0            0.00  \n",
      "7           1.79                1.0            0.00  \n",
      "12          1.50                1.0            0.00  \n",
      "13          2.16                1.0            0.20  \n",
      "17          1.78                1.0            0.20  \n",
      "19          1.55                1.0            0.90  \n",
      "25          2.65                1.0            0.15  \n",
      "26          2.40                1.0            3.80  \n",
      "30          2.58                1.0            2.15  \n",
      "37          2.95                1.0            0.50  \n",
      "45          2.45                1.0            0.00  \n",
      "46          2.00                1.0            2.00  \n",
      "47          1.50                1.0            2.00  \n",
      "49          2.75                1.0            0.00  \n",
      "51          2.90                1.0            0.90  \n",
      "62          3.15                1.0            2.10  \n",
      "63          2.58                1.0            0.60  \n",
      "68          2.40                1.0            0.50  \n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('train.csv')\n",
    "print(dataset.head())\n",
    "\n",
    "#only run when predicting humor or offensive\n",
    "# dataset = dataset[dataset['is_humor'] == 1]\n",
    "dataset = dataset[dataset['humor_controversy'] == 1]\n",
    "\n",
    "print(dataset.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"TENNESSEE: We're the best state. Nobody even comes close. *Elevennessee walks into the room* TENNESSEE: Oh shit...\"\n",
      " 'A man inserted an advertisement in the classifieds \"Wife Wanted\". The next day, he received 1000 of replies, all reading: \"You can have mine.\" Free delivery also available at your door step'\n",
      " \"Told my mom I hit 1200 Twitter followers. She pointed out how my brother owns a house and I'm wanted by several collection agencies. Oh ma!\"\n",
      " ...\n",
      " 'I live in quite a pretentious area. Even the ducks demand butter with their bread.'\n",
      " 'If you rearrange the letters of POSTMEN... They become VERY ANGRY.'\n",
      " \"Sins are like viruses, it's better you keep them to yourself\"]\n",
      "[0.2 1.1 0.  ... 0.  0.4 0.1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = dataset.iloc[:,1].values\n",
    "print(X)\n",
    "from sklearn.impute import SimpleImputer \n",
    "#print(humor_data.isnull().sum())\n",
    "constant_imputer=SimpleImputer(strategy='constant', fill_value=0)\n",
    "\n",
    "dataset.iloc[:]=constant_imputer.fit_transform(dataset)\n",
    "Y = dataset.iloc[:,5].values\n",
    "\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[161 167 123 ... 170  40   0]\n"
     ]
    }
   ],
   "source": [
    "# only do when trying for is_humor or is_offensive\n",
    "label_encoder_Y = LabelEncoder()\n",
    "Y = label_encoder_Y.fit_transform(Y)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training entries: 1972, test entries: 493\n",
      "TRAIN Dataset -> \n",
      "[\"I'm getting my girlfriend a prosthetic leg for Christmas It's a great stocking-filler.\"\n",
      " 'Anybody that eats Tide pods is an idiot. They could go to Costco and get the generic brand for half the price.'\n",
      " \"I left my Adderall in my Ford Fiesta. Now it's a @FordFocus\"\n",
      " 'Telescopes probably use mirrors which means there is absolutely no way to know how many vampires there are in space.'\n",
      " 'When it comes to tipping cows, I always like to leave at least a 15% gratuity.'\n",
      " 'A woman wakes up after a vaginal tuck to find three bunches of flowers beside her bed. One from her surgeon, to say all went well. One from her husband, \"get well soon\", and he loved her. One from Tommy in the burns unit, to say \"Thank you for my new ears\"'\n",
      " 'What do you call an overweight homosexual? Jigglypuff.'\n",
      " \"What's the hardest part about breaking up with a Japanese girl? You have to drop the bomb on her twice before she gets the point.\"\n",
      " \"Sorry, Babe, it's over. *I get on my motorcycle but I can't get it to start so I use my feet to scoot away*\"\n",
      " 'Do all black people have a problem with slavery? Or just mine?']\n",
      " Test Dataset ->\n",
      "[0.75 0.5  0.45 0.   0.3  1.   3.25 3.2  0.   3.4 ]\n"
     ]
    }
   ],
   "source": [
    "train_examples, test_examples, train_labels, test_labels = train_test_split(X,Y, test_size=0.2,random_state=0)\n",
    "print(\"Training entries: {}, test entries: {}\".format(len(train_examples), len(test_examples)))\n",
    "print(\"TRAIN Dataset -> \")\n",
    "print(train_examples[:10])\n",
    "print(\" Test Dataset ->\")\n",
    "print(train_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"green\">Build the model </font> <br>\n",
    "The neural network is created by stacking layers—this requires three main architectural decisions:\n",
    " * How to represent the text?\n",
    " * How many layers to use in the model?\n",
    " * How many *hidden units* to use for each layer? \n",
    "In this example, the input data consists of sentences. The labels to predict are either 0 or 1.\n",
    "\n",
    "One way to represent the text is to convert sentences into embeddings vectors. We can use a pre-trained text embedding as the first layer, which will have two advantages:\n",
    " *   we don't have to worry about text preprocessing,\n",
    " *   we can benefit from transfer learning.\n",
    " \n",
    "For this example we will use a model from [TensorFlow Hub] (https://www.tensorflow.org/hub) called [google/tf2-preview/gnews-swivel-20dim/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1).\n",
    "There are three other models to test for the sake of this tutorial:\n",
    " * [google/tf2-preview/gnews-swivel-20dim-with-oov/1] (https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1) - same as [google/tf2-preview/gnews-swivel-20dim/1] (https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1), but with 2.5% vocabulary converted to OOV buckets. This can help if vocabulary of the task and vocabulary of the model don't fully overlap.\n",
    " * [google/tf2-preview/nnlm-en-dim50/1] (https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1) - A much larger model with ~1M vocabulary size and 50 dimensions.\n",
    " * [google/tf2-preview/nnlm-en-dim128/1] (https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1) - Even larger model with ~1M vocabulary size and 128 dimensions.\n",
    "\n",
    "Let's first create a Keras layer that uses a TensorFlow Hub model to embed the sentences, and try it out on a couple of input examples. Note that the output shape of the produced embeddings is a expected: `(num_examples, embedding_dimension)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f9f26c17dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f9f26c17dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f9f3f9a1f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f9f3f9a1f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f9f3f987c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f9f3f987c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 50), dtype=float32, numpy=\n",
       "array([[ 1.72691688e-01, -1.96662605e-01,  1.53206140e-01,\n",
       "         3.01078483e-02,  3.61752331e-01,  8.19413960e-02,\n",
       "         2.16661379e-01, -1.41651854e-01, -6.67500272e-02,\n",
       "        -4.08735983e-02,  4.74083394e-01,  1.28887236e-01,\n",
       "         1.65047333e-01,  1.02559201e-01, -5.40219620e-02,\n",
       "        -2.26394057e-01,  2.50060298e-02,  2.27612212e-01,\n",
       "        -1.43635914e-01, -4.06409055e-01, -7.06835687e-02,\n",
       "        -2.16744497e-01, -1.23018719e-01, -1.13432370e-01,\n",
       "        -1.54559404e-01,  9.25091282e-02, -4.58936930e-01,\n",
       "         1.04611464e-01,  1.61529377e-01,  3.77314351e-02,\n",
       "        -3.20078969e-01,  1.31589726e-01, -9.86229256e-02,\n",
       "         8.90027285e-02, -1.37625471e-01,  9.96905938e-02,\n",
       "         2.76223779e-01,  1.68240607e-01,  1.69724807e-01,\n",
       "         5.58682904e-03,  3.67112011e-01, -1.56320259e-01,\n",
       "        -7.63619915e-02, -9.91256982e-02, -4.08143729e-01,\n",
       "        -1.38102518e-02, -6.91233203e-02,  1.59342177e-02,\n",
       "        -2.35531956e-01,  1.33103266e-01],\n",
       "       [ 2.63451934e-01,  8.95493105e-03,  1.02026731e-01,\n",
       "         1.16901666e-01,  2.81306982e-01, -1.78708509e-01,\n",
       "         2.47075260e-01,  2.53109753e-01,  9.46209766e-03,\n",
       "         6.40604421e-02,  8.83346051e-02,  2.47370258e-01,\n",
       "        -4.92685810e-02,  1.14041455e-01, -2.51911953e-02,\n",
       "        -1.41243950e-01, -6.78053912e-05,  1.50228709e-01,\n",
       "         1.73142403e-01, -3.17368567e-01,  1.14373267e-02,\n",
       "        -2.99491584e-01,  1.35980383e-01, -1.62324935e-01,\n",
       "        -2.16179356e-01,  3.06122035e-01, -4.80078399e-01,\n",
       "         3.61233950e-01, -9.57040042e-02,  5.69443218e-02,\n",
       "         8.68540853e-02,  2.21461609e-01,  1.24845751e-01,\n",
       "        -4.33900565e-01,  3.09831984e-02,  1.21855989e-01,\n",
       "         1.31566664e-02, -8.11476111e-02,  9.96146351e-02,\n",
       "        -1.83679655e-01, -8.11466575e-02,  3.18727791e-02,\n",
       "         3.81044745e-02,  2.22352636e-03, -5.87294400e-02,\n",
       "         2.10599795e-01, -1.08661167e-02, -5.76859713e-02,\n",
       "         1.94841679e-02,  2.28653520e-01],\n",
       "       [ 3.43426943e-01,  5.74289151e-02,  1.36201501e-01,\n",
       "         2.95805573e-01,  1.16571002e-01,  5.58936521e-02,\n",
       "         2.93226600e-01, -2.47022048e-01, -8.72643963e-02,\n",
       "         1.28096730e-01,  1.43434644e-01,  1.07513882e-01,\n",
       "        -2.97297761e-02,  4.92964871e-02, -1.87279940e-01,\n",
       "        -2.00430468e-01,  6.58744276e-02,  1.60348549e-01,\n",
       "        -1.73606411e-01, -1.64377347e-01, -3.18814837e-03,\n",
       "        -2.10915178e-01,  6.93512931e-02, -6.20015860e-02,\n",
       "         4.66929376e-02, -5.14173843e-02, -4.31656539e-01,\n",
       "        -7.10571781e-02,  3.57126743e-02,  8.06047991e-02,\n",
       "        -1.83494121e-01,  2.84381330e-01, -9.03760269e-02,\n",
       "        -6.63170293e-02, -1.31630257e-01,  2.57626504e-01,\n",
       "         3.13442677e-01,  1.30623519e-01,  1.98934693e-02,\n",
       "         2.37288810e-02,  2.50353098e-01, -6.10532500e-02,\n",
       "        -2.75675595e-01,  8.95052180e-02, -4.82222348e-01,\n",
       "        -9.62778553e-02, -2.15842083e-01, -2.71198511e-01,\n",
       "        -7.34730437e-02, -2.19909489e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\"\n",
    "hub_layer = hub.KerasLayer(model, output_shape=[50], input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)\n",
    "hub_layer(train_examples[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Let's now build the full model: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_5 (KerasLayer)   (None, 50)                48190600  \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                816       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 48,191,433\n",
      "Trainable params: 48,191,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\"> The layers are stacked sequentially to build the classifier: </font> <br>\n",
    " \n",
    " 1. The first layer is a TensorFlow Hub layer. This layer uses a pre-trained Saved Model to map a sentence into its embedding vector. The model that we are using ([google/tf2-preview/gnews-swivel-20dim/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1)) splits the sentence into tokens, embeds each token and then combines the embedding. The resulting dimensions are: `(num_examples, embedding_dimension)`.\n",
    " 2. This fixed-length output vector is piped through a fully-connected (`Dense`) layer with 16 hidden units.\n",
    " 3. The last layer is densely connected with a single output node. This outputs logits: the log-odds of the true class, according to the model.\n",
    "\n",
    "<font color=\"green\"> Hidden units </font> <br>\n",
    " \n",
    "The above model has two intermediate or \"hidden\" layers, between the input and output. \n",
    "The number of outputs (units, nodes, or neurons) is the dimension of the representational space for the layer. In other words, the amount of freedom the network is allowed when learning an internal representation.\n",
    " \n",
    "If a model has more hidden units (a higher-dimensional representation space), and/or more layers, then the network can learn more complex representations. However, it makes the network more computationally expensive and may lead to learning unwanted patterns—patterns that improve performance on training data but not on the test data. This is called *overfitting*, and we'll explore it later.\n",
    "\n",
    "<font color=\"green\">Loss function and optimizer </font> <br>\n",
    " \n",
    "A model needs a loss function and an optimizer for training. Since this is a binary classification problem and the model outputs a probability (a single-unit layer with a sigmoid activation), we'll use the `binary_crossentropy` loss function. \n",
    " \n",
    "This isn't the only choice for a loss function, you could, for instance, choose `mean_squared_error`. But, generally, `binary_crossentropy` is better for dealing with probabilities—it measures the \"distance\" between probability distributions, or in our case, between the ground-truth distribution and the predictions.\n",
    " \n",
    "Later, when we are exploring regression problems (say, to predict the price of a house), we will see how to use another loss function called mean squared error.\n",
    " \n",
    "<font color=\"green\">Now, configure the model to use an optimizer and a loss function: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Create a validation set </font>\n",
    "\n",
    "When training, we want to check the accuracy of the model on data it hasn't seen before. <br>\n",
    "Create a *validation set* by setting apart 10,000 examples from the original training data. <br> \n",
    "(Why not use the testing set now? Our goal is to develop and tune our model using only the training data, then use the test data just once to evaluate our accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1972\n"
     ]
    }
   ],
   "source": [
    "print(len(train_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = train_examples[:1450]\n",
    "partial_x_train = train_examples[1450:]\n",
    "\n",
    "y_val = train_labels[:1450]\n",
    "partial_y_train = train_labels[1450:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Train the model </font> <br>\n",
    "Train the model for 40 epochs in mini-batches of 512 samples. This is 40 iterations over all samples in the `x_train` and `y_train` tensors. While training, monitor the model's loss and accuracy on the 10,000 samples from the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2/2 [==============================] - 1s 607ms/step - loss: 0.6260 - accuracy: 0.0230 - val_loss: 0.6210 - val_accuracy: 0.0186\n",
      "Epoch 2/15\n",
      "2/2 [==============================] - 1s 634ms/step - loss: 0.5966 - accuracy: 0.0211 - val_loss: 0.6113 - val_accuracy: 0.0159\n",
      "Epoch 3/15\n",
      "2/2 [==============================] - 1s 528ms/step - loss: 0.5716 - accuracy: 0.0192 - val_loss: 0.6030 - val_accuracy: 0.0138\n",
      "Epoch 4/15\n",
      "2/2 [==============================] - 1s 652ms/step - loss: 0.5485 - accuracy: 0.0192 - val_loss: 0.5956 - val_accuracy: 0.0124\n",
      "Epoch 5/15\n",
      "2/2 [==============================] - 1s 532ms/step - loss: 0.5274 - accuracy: 0.0172 - val_loss: 0.5891 - val_accuracy: 0.0124\n",
      "Epoch 6/15\n",
      "2/2 [==============================] - 1s 539ms/step - loss: 0.5072 - accuracy: 0.0172 - val_loss: 0.5823 - val_accuracy: 0.0124\n",
      "Epoch 7/15\n",
      "2/2 [==============================] - 1s 598ms/step - loss: 0.4869 - accuracy: 0.0172 - val_loss: 0.5745 - val_accuracy: 0.0131\n",
      "Epoch 8/15\n",
      "2/2 [==============================] - 1s 609ms/step - loss: 0.4659 - accuracy: 0.0172 - val_loss: 0.5663 - val_accuracy: 0.0124\n",
      "Epoch 9/15\n",
      "2/2 [==============================] - 1s 538ms/step - loss: 0.4438 - accuracy: 0.0172 - val_loss: 0.5579 - val_accuracy: 0.0117\n",
      "Epoch 10/15\n",
      "2/2 [==============================] - 1s 634ms/step - loss: 0.4217 - accuracy: 0.0192 - val_loss: 0.5499 - val_accuracy: 0.0117\n",
      "Epoch 11/15\n",
      "2/2 [==============================] - 1s 559ms/step - loss: 0.3994 - accuracy: 0.0192 - val_loss: 0.5423 - val_accuracy: 0.0117\n",
      "Epoch 12/15\n",
      "2/2 [==============================] - 1s 539ms/step - loss: 0.3774 - accuracy: 0.0211 - val_loss: 0.5346 - val_accuracy: 0.0117\n",
      "Epoch 13/15\n",
      "2/2 [==============================] - 1s 550ms/step - loss: 0.3551 - accuracy: 0.0230 - val_loss: 0.5272 - val_accuracy: 0.0117\n",
      "Epoch 14/15\n",
      "2/2 [==============================] - 1s 672ms/step - loss: 0.3323 - accuracy: 0.0230 - val_loss: 0.5197 - val_accuracy: 0.0110\n",
      "Epoch 15/15\n",
      "2/2 [==============================] - 1s 568ms/step - loss: 0.3092 - accuracy: 0.0230 - val_loss: 0.5118 - val_accuracy: 0.0110\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=15,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\"> Evaluate the model </font> <br>\n",
    "And let's see how the model performs. Two values will be returned. <br>\n",
    "-> Loss (a number which represents our error, lower values are better), and <br>\n",
    "-> accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 2s 129ms/step - loss: 0.4824 - accuracy: 0.0101\n",
      "*******Codalab************ [0.4824236333370209, 0.010141988284885883]\n",
      "[1.4  0.   0.3  0.2  0.2  0.5  0.05 0.35 0.   1.5  1.45 0.   0.   2.6\n",
      " 1.95 3.25 2.55 0.25 0.2  2.45 0.2  0.2  0.25 0.55 0.   0.2  0.25 2.7\n",
      " 0.   0.   0.1  0.5  2.85 0.2  0.1  1.65 1.8  0.   0.   0.   0.3  0.05\n",
      " 0.   0.   0.65 4.1  0.15 2.85 0.1  2.7  3.45 1.15 0.05 0.7  0.25 0.2\n",
      " 0.1  0.   0.15 0.05 0.5  1.3  0.05 0.1  0.35 0.2  2.85 0.95 3.15 1.05\n",
      " 0.   1.25 0.2  3.85 0.55 0.25 0.1  0.15 0.05 1.45 0.45 2.7  1.7  3.1\n",
      " 4.05 0.   0.   0.   0.6  0.   0.2  0.05 1.75 0.3  0.   2.   0.05 0.\n",
      " 0.   0.1  2.15 0.05 2.35 3.65 0.15 1.1  0.5  3.8  0.   1.65 0.   3.55\n",
      " 1.3  1.05 0.   0.   0.2  0.3  0.4  0.05 0.55 0.   0.   2.05 0.   0.\n",
      " 0.   0.05 0.   0.   0.25 3.35 3.4  0.05 0.   0.4  0.15 0.25 2.25 0.\n",
      " 3.6  2.45 0.   0.2  4.2  0.25 0.   0.   0.6  0.95 1.2  0.4  0.5  1.45\n",
      " 0.35 0.25 0.85 2.55 0.2  0.3  0.15 2.25 0.   0.2  2.5  0.   0.05 1.65\n",
      " 0.   0.   1.7  0.   0.   3.6  2.55 1.55 1.85 0.5  0.55 1.1  0.3  0.2\n",
      " 2.9  2.7  1.6  2.1  2.   3.25 0.   0.9  2.1  0.4  2.8  2.5  0.4  2.8\n",
      " 0.25 0.35 0.8  0.05 0.   1.35 0.2  1.5  2.   2.25 0.15 1.35 0.   2.\n",
      " 1.95 0.2  0.1  0.95 2.15 0.   1.   1.2  2.05 1.95 2.1  0.35 0.   0.55\n",
      " 0.95 0.1  0.25 0.3  2.85 0.4  0.   0.1  0.05 0.25 0.15 2.1  0.2  2.4\n",
      " 0.   1.65 0.   1.45 1.   0.   0.35 0.   0.4  0.   0.1  0.15 0.   0.\n",
      " 0.   0.   3.95 3.85 1.9  1.65 0.3  0.9  0.5  0.05 0.2  0.1  0.   2.2\n",
      " 0.   0.4  0.   0.   0.   0.3  0.   3.65 0.7  2.1  2.15 1.1  0.6  0.\n",
      " 1.55 3.6  0.2  2.5  0.   0.   1.35 1.15 0.3  0.25 0.95 2.65 1.4  0.\n",
      " 1.   0.45 0.15 2.55 0.6  0.05 0.   0.   2.15 0.   0.   0.2  0.85 0.85\n",
      " 0.95 0.4  2.6  2.7  1.   0.   0.6  0.   0.1  0.2  0.   0.9  2.7  1.4\n",
      " 0.3  0.25 2.65 0.8  0.2  0.25 0.   0.15 1.15 0.1  2.65 0.15 2.1  0.\n",
      " 2.7  0.6  0.5  1.6  0.1  0.9  2.85 3.1  0.15 0.   2.1  1.45 0.15 0.15\n",
      " 0.6  0.1  1.25 0.   0.1  0.   0.15 0.2  1.2  0.05 0.   0.2  0.05 4.15\n",
      " 1.1  0.75 1.1  0.   2.55 0.   0.   0.35 0.2  0.3  0.6  0.   0.   0.\n",
      " 0.   0.8  0.   0.   0.35 0.25 0.   1.5  1.95 0.15 2.7  3.7  0.05 0.8\n",
      " 1.55 0.4  1.75 2.2  1.7  2.15 1.8  0.75 0.05 0.1  0.1  0.   0.   1.45\n",
      " 0.05 1.45 0.05 3.5  2.9  0.55 0.1  0.   0.   0.9  1.05 3.4  3.35 0.2\n",
      " 2.   2.25 0.1  0.65 1.6  0.   0.15 0.   1.2  2.15 0.   0.3  0.   1.5\n",
      " 1.   0.   0.   3.45 0.   0.15 0.2  0.   0.1  0.   3.   1.95 1.05 0.85\n",
      " 1.95 1.2  0.1  0.   0.   0.7  0.   0.15 0.   0.75 0.   0.35 2.6  0.\n",
      " 0.3  0.   0.85 0.6  0.75 1.95 2.75 0.   1.45 0.05 0.   0.3  0.   0.35\n",
      " 0.15 1.55 0.1  0.85 3.5  0.1  0.05 0.15 0.1  0.1  2.8  0.75 0.05 2.\n",
      " 0.15 0.   2.4 ]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_examples, test_labels)\n",
    "\n",
    "print(\"*******Codalab************\",results)\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.71463233]\n",
      " [ 0.466227  ]\n",
      " [ 0.43464866]\n",
      " [ 0.5293789 ]\n",
      " [ 0.6286638 ]\n",
      " [ 0.30118436]\n",
      " [ 0.6193511 ]\n",
      " [ 0.95704293]\n",
      " [ 0.7478211 ]\n",
      " [ 0.43221325]\n",
      " [ 0.83511585]\n",
      " [ 0.5363073 ]\n",
      " [ 0.4228049 ]\n",
      " [ 0.86665577]\n",
      " [ 0.43635595]\n",
      " [ 0.17332546]\n",
      " [ 0.53414094]\n",
      " [ 0.45308346]\n",
      " [ 0.6421553 ]\n",
      " [ 0.02027299]\n",
      " [ 0.38690934]\n",
      " [ 0.26311034]\n",
      " [ 0.87128294]\n",
      " [ 0.33200538]\n",
      " [ 0.5597373 ]\n",
      " [ 0.555234  ]\n",
      " [ 0.5751617 ]\n",
      " [ 0.5611706 ]\n",
      " [ 0.5191389 ]\n",
      " [ 0.44830987]\n",
      " [ 0.37261343]\n",
      " [ 0.58720833]\n",
      " [ 0.5899044 ]\n",
      " [ 0.7572112 ]\n",
      " [ 0.29644203]\n",
      " [ 1.0250517 ]\n",
      " [ 0.5845968 ]\n",
      " [ 0.33077806]\n",
      " [ 0.67161405]\n",
      " [ 0.5704586 ]\n",
      " [ 0.42673382]\n",
      " [ 0.4787372 ]\n",
      " [ 0.5068246 ]\n",
      " [ 0.2629834 ]\n",
      " [ 0.1727949 ]\n",
      " [ 0.7206178 ]\n",
      " [ 0.33040145]\n",
      " [ 0.51743907]\n",
      " [ 0.2683318 ]\n",
      " [ 0.70921516]\n",
      " [ 0.8130994 ]\n",
      " [ 0.40617532]\n",
      " [ 0.32237297]\n",
      " [ 0.51842517]\n",
      " [ 0.5116469 ]\n",
      " [ 0.5600267 ]\n",
      " [ 0.51031935]\n",
      " [ 0.46783584]\n",
      " [ 0.9689901 ]\n",
      " [ 0.5859816 ]\n",
      " [ 0.3487292 ]\n",
      " [ 0.23320568]\n",
      " [ 0.6203511 ]\n",
      " [ 0.7501102 ]\n",
      " [ 0.6104374 ]\n",
      " [ 0.80066204]\n",
      " [ 0.25312746]\n",
      " [ 0.37512097]\n",
      " [ 0.12652475]\n",
      " [ 0.7649709 ]\n",
      " [ 0.4499385 ]\n",
      " [ 0.916028  ]\n",
      " [ 0.61808854]\n",
      " [ 0.97077876]\n",
      " [ 1.5198821 ]\n",
      " [ 0.48097432]\n",
      " [ 0.7572388 ]\n",
      " [ 0.32888997]\n",
      " [ 0.53183204]\n",
      " [ 0.30134413]\n",
      " [ 0.524349  ]\n",
      " [ 0.5404148 ]\n",
      " [ 0.84546685]\n",
      " [ 0.26987785]\n",
      " [ 0.38601333]\n",
      " [ 0.23680799]\n",
      " [ 0.44103825]\n",
      " [ 0.7507548 ]\n",
      " [ 1.1322055 ]\n",
      " [ 0.86374617]\n",
      " [ 0.7391016 ]\n",
      " [ 0.90799403]\n",
      " [ 0.7332568 ]\n",
      " [ 0.5814266 ]\n",
      " [ 0.62877566]\n",
      " [ 0.14471418]\n",
      " [ 0.1987424 ]\n",
      " [ 0.4166115 ]\n",
      " [ 0.54519004]\n",
      " [ 0.15739153]\n",
      " [ 0.23567821]\n",
      " [ 0.3051163 ]\n",
      " [ 0.5254662 ]\n",
      " [ 0.8506402 ]\n",
      " [ 0.3983803 ]\n",
      " [ 0.2800739 ]\n",
      " [ 0.9515102 ]\n",
      " [ 0.35966343]\n",
      " [ 0.6455914 ]\n",
      " [ 0.6254227 ]\n",
      " [ 0.7776499 ]\n",
      " [ 0.9270312 ]\n",
      " [ 0.13838635]\n",
      " [ 0.6566226 ]\n",
      " [ 0.29319015]\n",
      " [ 0.02127569]\n",
      " [ 0.9488485 ]\n",
      " [ 0.8320296 ]\n",
      " [ 0.1469797 ]\n",
      " [ 0.44155568]\n",
      " [ 0.5337797 ]\n",
      " [ 0.89657426]\n",
      " [ 0.20951624]\n",
      " [ 0.7177329 ]\n",
      " [ 0.7428999 ]\n",
      " [ 0.2135152 ]\n",
      " [ 0.5754504 ]\n",
      " [ 0.54062855]\n",
      " [ 1.1220545 ]\n",
      " [ 0.7110819 ]\n",
      " [ 1.0435146 ]\n",
      " [ 0.8218125 ]\n",
      " [ 0.65954745]\n",
      " [ 0.6108628 ]\n",
      " [ 0.30684304]\n",
      " [ 0.60936546]\n",
      " [ 0.17836533]\n",
      " [ 0.42879552]\n",
      " [ 0.3426816 ]\n",
      " [ 0.6422542 ]\n",
      " [ 0.6534327 ]\n",
      " [ 0.15204497]\n",
      " [ 0.4321615 ]\n",
      " [ 0.29727703]\n",
      " [ 0.57410425]\n",
      " [ 0.12138163]\n",
      " [ 0.21326558]\n",
      " [ 0.16330475]\n",
      " [ 0.1494412 ]\n",
      " [ 0.84136105]\n",
      " [ 0.6560687 ]\n",
      " [ 0.52678645]\n",
      " [ 0.49395758]\n",
      " [ 1.1510329 ]\n",
      " [ 0.616276  ]\n",
      " [ 0.7985661 ]\n",
      " [ 0.6621012 ]\n",
      " [ 0.7216016 ]\n",
      " [ 0.5251738 ]\n",
      " [ 0.33139592]\n",
      " [ 0.76286685]\n",
      " [ 0.3173465 ]\n",
      " [ 0.09513809]\n",
      " [ 0.36908048]\n",
      " [ 0.6733763 ]\n",
      " [ 0.28939775]\n",
      " [ 0.33070624]\n",
      " [ 0.2534113 ]\n",
      " [ 0.61706144]\n",
      " [ 0.8167056 ]\n",
      " [ 0.43249935]\n",
      " [ 0.5374416 ]\n",
      " [ 1.2171848 ]\n",
      " [ 0.35395747]\n",
      " [ 1.0490432 ]\n",
      " [ 1.0835631 ]\n",
      " [ 0.5682747 ]\n",
      " [ 0.6441429 ]\n",
      " [ 0.79343325]\n",
      " [ 0.85054874]\n",
      " [ 0.9446893 ]\n",
      " [ 0.4760629 ]\n",
      " [ 0.10167602]\n",
      " [ 0.69062245]\n",
      " [ 0.7353049 ]\n",
      " [ 0.40291592]\n",
      " [ 0.40895534]\n",
      " [ 0.6125211 ]\n",
      " [ 0.48917526]\n",
      " [ 0.59355736]\n",
      " [ 0.49948785]\n",
      " [ 0.589579  ]\n",
      " [ 0.8145673 ]\n",
      " [ 0.6894296 ]\n",
      " [ 0.53853065]\n",
      " [ 0.75623095]\n",
      " [ 0.56479406]\n",
      " [ 0.3095237 ]\n",
      " [ 0.45154762]\n",
      " [ 0.73904014]\n",
      " [ 0.440409  ]\n",
      " [ 0.34989694]\n",
      " [ 0.52813184]\n",
      " [ 0.33167416]\n",
      " [ 0.6667253 ]\n",
      " [ 0.51356524]\n",
      " [ 0.73631155]\n",
      " [ 0.69964105]\n",
      " [ 0.571654  ]\n",
      " [ 1.1243953 ]\n",
      " [ 0.83539116]\n",
      " [ 0.31400663]\n",
      " [ 0.33647576]\n",
      " [ 0.5859485 ]\n",
      " [ 0.8094928 ]\n",
      " [ 0.6260703 ]\n",
      " [ 0.67228866]\n",
      " [ 0.4600282 ]\n",
      " [ 0.7700167 ]\n",
      " [ 0.74882066]\n",
      " [ 0.4123844 ]\n",
      " [ 0.3095146 ]\n",
      " [ 0.48003352]\n",
      " [ 0.26961744]\n",
      " [ 0.40723878]\n",
      " [ 0.3121245 ]\n",
      " [ 0.18517835]\n",
      " [ 0.7754424 ]\n",
      " [ 0.76322734]\n",
      " [ 0.51333845]\n",
      " [ 0.7591017 ]\n",
      " [ 0.71300983]\n",
      " [ 0.39473665]\n",
      " [ 0.6577942 ]\n",
      " [ 0.6037127 ]\n",
      " [ 0.42535502]\n",
      " [ 0.36529177]\n",
      " [ 0.84063435]\n",
      " [ 0.11438248]\n",
      " [ 0.5800841 ]\n",
      " [ 0.6898317 ]\n",
      " [ 0.3516814 ]\n",
      " [ 0.60283643]\n",
      " [ 0.59555006]\n",
      " [ 0.4545334 ]\n",
      " [ 0.49371582]\n",
      " [ 0.9621471 ]\n",
      " [ 0.91416526]\n",
      " [ 0.54182774]\n",
      " [ 0.70103323]\n",
      " [ 0.32077768]\n",
      " [ 0.4735967 ]\n",
      " [ 0.41383952]\n",
      " [ 0.73881334]\n",
      " [ 0.58467937]\n",
      " [ 0.8850627 ]\n",
      " [ 0.56177294]\n",
      " [ 0.63264334]\n",
      " [ 0.44002506]\n",
      " [ 0.5634033 ]\n",
      " [ 0.5592455 ]\n",
      " [ 0.78148156]\n",
      " [ 0.60581756]\n",
      " [ 0.9076333 ]\n",
      " [ 0.7442875 ]\n",
      " [ 0.5039125 ]\n",
      " [ 0.30675456]\n",
      " [ 0.20442326]\n",
      " [ 0.81314015]\n",
      " [ 0.48114726]\n",
      " [ 0.763252  ]\n",
      " [ 0.8362066 ]\n",
      " [ 0.43177497]\n",
      " [ 0.25129497]\n",
      " [ 0.4813041 ]\n",
      " [ 0.35262883]\n",
      " [ 0.3423215 ]\n",
      " [ 1.0228114 ]\n",
      " [ 0.33007646]\n",
      " [ 0.3636307 ]\n",
      " [ 0.4767476 ]\n",
      " [ 0.52222455]\n",
      " [ 0.27909768]\n",
      " [ 1.3191458 ]\n",
      " [ 0.96173143]\n",
      " [ 0.3846217 ]\n",
      " [ 0.39417636]\n",
      " [ 0.46931708]\n",
      " [ 0.9243719 ]\n",
      " [ 0.78820133]\n",
      " [ 0.4999315 ]\n",
      " [ 0.7965777 ]\n",
      " [ 0.87261677]\n",
      " [ 0.96003044]\n",
      " [ 0.33941868]\n",
      " [ 0.59588414]\n",
      " [ 0.17129181]\n",
      " [ 0.65848964]\n",
      " [ 0.7148702 ]\n",
      " [ 0.6780851 ]\n",
      " [ 0.52373385]\n",
      " [ 0.50637084]\n",
      " [ 0.7503251 ]\n",
      " [ 0.55864346]\n",
      " [ 0.83656377]\n",
      " [ 0.6218963 ]\n",
      " [ 1.2040802 ]\n",
      " [ 0.48529273]\n",
      " [ 0.87173223]\n",
      " [ 0.66044295]\n",
      " [ 0.8966657 ]\n",
      " [ 0.6824763 ]\n",
      " [ 0.76811373]\n",
      " [ 0.41534984]\n",
      " [ 0.6448955 ]\n",
      " [ 0.6263097 ]\n",
      " [ 0.35094395]\n",
      " [ 0.8414421 ]\n",
      " [ 0.5738741 ]\n",
      " [ 0.38990614]\n",
      " [ 0.4416047 ]\n",
      " [ 0.29944494]\n",
      " [ 0.5286689 ]\n",
      " [ 0.06672755]\n",
      " [ 0.57404244]\n",
      " [ 0.3098408 ]\n",
      " [ 0.63023317]\n",
      " [ 0.3447076 ]\n",
      " [ 1.0016748 ]\n",
      " [ 0.6349784 ]\n",
      " [ 0.18561211]\n",
      " [ 1.0763549 ]\n",
      " [ 0.2669721 ]\n",
      " [ 0.51141864]\n",
      " [ 0.4560756 ]\n",
      " [ 0.16296738]\n",
      " [ 0.8894282 ]\n",
      " [ 0.6382797 ]\n",
      " [ 0.43324468]\n",
      " [ 0.23653646]\n",
      " [ 1.0466522 ]\n",
      " [ 1.0134385 ]\n",
      " [ 0.7136344 ]\n",
      " [ 0.6654942 ]\n",
      " [ 0.70210886]\n",
      " [ 0.12286934]\n",
      " [ 0.705279  ]\n",
      " [ 0.20647559]\n",
      " [ 0.5974566 ]\n",
      " [ 0.46022695]\n",
      " [ 0.53578544]\n",
      " [ 0.44197702]\n",
      " [ 0.7406144 ]\n",
      " [ 1.1552165 ]\n",
      " [ 0.59599245]\n",
      " [ 0.3557064 ]\n",
      " [ 0.44132903]\n",
      " [ 0.7296624 ]\n",
      " [ 0.49179912]\n",
      " [ 0.17013788]\n",
      " [ 0.7138654 ]\n",
      " [ 0.74644196]\n",
      " [ 0.39748248]\n",
      " [ 0.48367468]\n",
      " [ 0.45369703]\n",
      " [ 0.26764056]\n",
      " [ 0.47516683]\n",
      " [ 0.43131402]\n",
      " [ 0.3215995 ]\n",
      " [ 0.8038509 ]\n",
      " [ 0.3989661 ]\n",
      " [ 1.084079  ]\n",
      " [ 0.1366159 ]\n",
      " [ 0.09825476]\n",
      " [ 0.4861251 ]\n",
      " [ 0.07312544]\n",
      " [ 0.69569683]\n",
      " [ 0.36152503]\n",
      " [ 0.54978204]\n",
      " [ 0.4519971 ]\n",
      " [ 0.46468967]\n",
      " [ 0.73478484]\n",
      " [ 0.51369566]\n",
      " [ 0.6189972 ]\n",
      " [ 0.6588748 ]\n",
      " [ 0.42300928]\n",
      " [ 0.50988644]\n",
      " [ 0.6314018 ]\n",
      " [ 0.50001913]\n",
      " [ 0.5475148 ]\n",
      " [ 0.42429972]\n",
      " [ 0.3926274 ]\n",
      " [ 0.41293085]\n",
      " [ 0.3209427 ]\n",
      " [ 0.51494944]\n",
      " [ 0.38840866]\n",
      " [ 0.60024714]\n",
      " [ 0.564743  ]\n",
      " [ 0.6157357 ]\n",
      " [ 0.4554803 ]\n",
      " [ 1.1705469 ]\n",
      " [ 0.9857444 ]\n",
      " [ 0.6001477 ]\n",
      " [ 0.9301555 ]\n",
      " [ 0.6753113 ]\n",
      " [ 0.721929  ]\n",
      " [ 0.363455  ]\n",
      " [ 0.49052626]\n",
      " [ 0.5206347 ]\n",
      " [ 1.0383259 ]\n",
      " [ 0.55214965]\n",
      " [ 0.46520966]\n",
      " [ 0.9462118 ]\n",
      " [ 0.72242546]\n",
      " [ 0.6294786 ]\n",
      " [ 0.9047749 ]\n",
      " [ 0.37588084]\n",
      " [ 0.7460058 ]\n",
      " [ 0.7481633 ]\n",
      " [ 0.81068426]\n",
      " [ 0.67539215]\n",
      " [ 0.5931819 ]\n",
      " [ 0.94057614]\n",
      " [ 0.36627263]\n",
      " [ 0.47442   ]\n",
      " [ 0.84853816]\n",
      " [ 0.3118994 ]\n",
      " [ 0.32165295]\n",
      " [ 0.50490135]\n",
      " [ 0.48424482]\n",
      " [ 0.8053835 ]\n",
      " [ 0.73330474]\n",
      " [ 0.396422  ]\n",
      " [ 0.34896123]\n",
      " [ 1.1056591 ]\n",
      " [ 0.39477214]\n",
      " [ 0.72766984]\n",
      " [ 0.5135357 ]\n",
      " [ 0.56161535]\n",
      " [ 0.44942108]\n",
      " [ 0.44511417]\n",
      " [ 0.7594204 ]\n",
      " [ 0.6401163 ]\n",
      " [ 0.6300001 ]\n",
      " [ 0.8022307 ]\n",
      " [ 0.7954041 ]\n",
      " [ 0.4173813 ]\n",
      " [ 0.34009996]\n",
      " [ 0.6674798 ]\n",
      " [ 0.55452645]\n",
      " [ 0.56677276]\n",
      " [ 0.29671866]\n",
      " [ 0.69263506]\n",
      " [ 0.65417534]\n",
      " [ 0.32096967]\n",
      " [ 0.40098026]\n",
      " [ 0.55857325]\n",
      " [ 0.7307818 ]\n",
      " [ 0.31953448]\n",
      " [ 0.44078127]\n",
      " [ 0.5436281 ]\n",
      " [ 0.6397779 ]\n",
      " [ 0.4726931 ]\n",
      " [ 0.47173333]\n",
      " [ 0.29043508]\n",
      " [ 0.2989992 ]\n",
      " [ 0.4700541 ]\n",
      " [ 0.6237142 ]\n",
      " [ 0.629729  ]\n",
      " [ 0.7008346 ]\n",
      " [ 0.6529277 ]\n",
      " [ 0.47691375]\n",
      " [ 0.73861206]\n",
      " [ 0.9825152 ]\n",
      " [ 0.76560974]\n",
      " [ 0.38717145]\n",
      " [ 0.54245174]\n",
      " [ 0.6220983 ]\n",
      " [ 0.48271844]\n",
      " [ 0.60584295]\n",
      " [ 0.52565855]\n",
      " [ 0.43326807]\n",
      " [ 0.42888337]\n",
      " [ 0.4413181 ]\n",
      " [ 0.62865424]\n",
      " [ 0.67871857]\n",
      " [ 0.6284976 ]\n",
      " [ 0.67835164]\n",
      " [ 0.411901  ]\n",
      " [ 0.52531487]\n",
      " [ 0.7308283 ]\n",
      " [ 0.5942439 ]\n",
      " [ 0.6117791 ]\n",
      " [ 1.1926433 ]\n",
      " [ 0.89440364]\n",
      " [ 0.5943496 ]\n",
      " [ 0.642686  ]\n",
      " [ 0.5074145 ]\n",
      " [ 0.71475714]\n",
      " [ 0.7945845 ]\n",
      " [ 0.49727756]\n",
      " [ 0.60023487]\n",
      " [ 1.107748  ]\n",
      " [ 0.9329715 ]\n",
      " [ 0.7203267 ]\n",
      " [ 0.61384463]\n",
      " [ 1.0364817 ]\n",
      " [ 0.7108178 ]\n",
      " [ 0.40020972]\n",
      " [ 0.49838245]\n",
      " [ 0.6891608 ]\n",
      " [ 0.56119806]\n",
      " [ 0.9602291 ]\n",
      " [ 0.87931174]\n",
      " [ 0.6858274 ]\n",
      " [ 0.39186192]\n",
      " [ 0.5672486 ]\n",
      " [ 0.45877373]\n",
      " [ 0.772926  ]\n",
      " [ 0.813704  ]\n",
      " [ 0.6222287 ]\n",
      " [ 0.33957613]\n",
      " [ 0.83798623]\n",
      " [ 0.45978987]\n",
      " [ 0.5445262 ]\n",
      " [ 0.82515377]\n",
      " [ 0.79594356]\n",
      " [ 0.47290412]\n",
      " [ 0.42603728]\n",
      " [ 0.54737854]\n",
      " [ 0.5652505 ]\n",
      " [ 0.5819737 ]\n",
      " [ 0.38303357]\n",
      " [ 0.65522265]\n",
      " [ 1.1323677 ]\n",
      " [ 0.8037579 ]\n",
      " [ 0.36190963]\n",
      " [ 0.92452514]\n",
      " [ 0.6899696 ]\n",
      " [ 0.26324075]\n",
      " [ 0.5732256 ]\n",
      " [ 0.3734098 ]\n",
      " [ 0.5361466 ]\n",
      " [ 0.55699503]\n",
      " [ 0.28758729]\n",
      " [ 0.5460557 ]\n",
      " [ 1.075265  ]\n",
      " [ 0.6788966 ]\n",
      " [ 0.4079473 ]\n",
      " [ 0.6190171 ]\n",
      " [ 0.4108885 ]\n",
      " [ 0.9289551 ]\n",
      " [ 1.0531987 ]\n",
      " [ 0.68437386]\n",
      " [ 0.81355524]\n",
      " [ 0.80170643]\n",
      " [ 0.6400967 ]\n",
      " [ 0.41938668]\n",
      " [ 0.30899853]\n",
      " [ 0.2766247 ]\n",
      " [ 0.28098977]\n",
      " [ 0.10635577]\n",
      " [ 0.6526289 ]\n",
      " [ 0.6075189 ]\n",
      " [ 0.3655459 ]\n",
      " [ 0.33116773]\n",
      " [ 0.9236419 ]\n",
      " [ 0.73558056]\n",
      " [ 0.38659438]\n",
      " [ 0.45650524]\n",
      " [ 0.7931905 ]\n",
      " [ 0.3875561 ]\n",
      " [ 0.61741287]\n",
      " [ 0.6136075 ]\n",
      " [ 0.5393326 ]\n",
      " [ 0.3246374 ]\n",
      " [ 0.5079174 ]\n",
      " [ 0.04342503]\n",
      " [ 0.5067402 ]\n",
      " [ 0.29120225]\n",
      " [ 0.5364494 ]\n",
      " [ 0.9196981 ]\n",
      " [ 0.34382468]\n",
      " [ 0.59600306]\n",
      " [ 0.23020999]\n",
      " [ 0.14227779]\n",
      " [ 1.0009296 ]\n",
      " [ 0.22734503]\n",
      " [ 0.14048646]\n",
      " [ 0.6057465 ]\n",
      " [ 0.37345645]\n",
      " [ 1.4860815 ]\n",
      " [ 0.80740565]\n",
      " [ 0.64127487]\n",
      " [ 0.65240824]\n",
      " [ 0.34698814]\n",
      " [ 0.52041006]\n",
      " [ 0.58443636]\n",
      " [ 0.54021066]\n",
      " [ 0.68417513]\n",
      " [ 0.47103775]\n",
      " [ 0.2672689 ]\n",
      " [ 0.8494611 ]\n",
      " [ 0.56959295]\n",
      " [ 0.56107044]\n",
      " [ 0.43598023]\n",
      " [ 0.71190846]\n",
      " [ 0.26780394]\n",
      " [ 0.4767317 ]\n",
      " [ 0.36777133]\n",
      " [ 0.378623  ]\n",
      " [ 0.53548217]\n",
      " [ 0.46080983]\n",
      " [ 0.77230906]\n",
      " [ 0.45812905]\n",
      " [ 0.9083637 ]\n",
      " [ 0.5324098 ]\n",
      " [ 1.0064106 ]\n",
      " [ 0.72222584]\n",
      " [ 0.35855928]\n",
      " [ 0.72243804]\n",
      " [ 0.82443476]\n",
      " [ 0.5840508 ]\n",
      " [ 0.97227764]\n",
      " [ 0.3114922 ]\n",
      " [ 0.5284109 ]\n",
      " [ 0.8063226 ]\n",
      " [ 0.7466629 ]\n",
      " [ 0.57028604]\n",
      " [ 0.21187063]\n",
      " [ 0.89199054]\n",
      " [ 0.8905626 ]\n",
      " [ 0.3095851 ]\n",
      " [ 0.2703718 ]\n",
      " [ 0.6696203 ]\n",
      " [ 0.3902613 ]\n",
      " [ 0.55478275]\n",
      " [ 0.21522231]\n",
      " [ 0.18687068]\n",
      " [ 0.8256113 ]\n",
      " [ 0.8164475 ]\n",
      " [ 0.8439425 ]\n",
      " [ 1.0320001 ]\n",
      " [ 0.47059944]\n",
      " [ 0.5759507 ]\n",
      " [ 0.7038803 ]\n",
      " [ 0.43396318]\n",
      " [ 0.43488947]\n",
      " [ 0.9817515 ]\n",
      " [ 0.34563863]\n",
      " [ 0.45212302]\n",
      " [ 0.28733227]\n",
      " [ 0.66701806]\n",
      " [ 0.506821  ]\n",
      " [ 0.8051656 ]\n",
      " [ 0.740536  ]\n",
      " [ 0.77988446]\n",
      " [ 0.7224845 ]\n",
      " [ 0.66080475]\n",
      " [ 0.6143031 ]\n",
      " [ 0.6845118 ]\n",
      " [ 0.47331372]\n",
      " [ 0.65870893]\n",
      " [ 0.8877018 ]\n",
      " [ 0.90261626]\n",
      " [ 0.5831131 ]\n",
      " [ 0.43463534]\n",
      " [ 0.5434541 ]\n",
      " [ 0.39878836]\n",
      " [ 0.43821514]\n",
      " [ 0.87951964]\n",
      " [ 0.6546079 ]\n",
      " [ 0.31155267]\n",
      " [ 0.535766  ]\n",
      " [ 0.6697166 ]\n",
      " [ 0.61786497]\n",
      " [ 0.64739007]\n",
      " [ 0.67334074]\n",
      " [ 0.4728004 ]\n",
      " [ 0.8706361 ]\n",
      " [ 0.8132169 ]\n",
      " [ 0.45925233]\n",
      " [ 0.670107  ]\n",
      " [ 0.7214527 ]\n",
      " [ 0.5835343 ]\n",
      " [ 0.7125502 ]\n",
      " [ 0.46990645]\n",
      " [ 0.82062465]\n",
      " [ 0.7570512 ]\n",
      " [ 0.5732835 ]\n",
      " [ 0.292735  ]\n",
      " [ 0.35046002]\n",
      " [ 0.6873214 ]\n",
      " [ 0.5525069 ]\n",
      " [ 0.72843015]\n",
      " [ 0.5982567 ]\n",
      " [ 0.5287448 ]\n",
      " [ 0.46965063]\n",
      " [ 0.6104019 ]\n",
      " [ 0.34206134]\n",
      " [ 0.8271035 ]\n",
      " [ 1.2724888 ]\n",
      " [ 0.61311054]\n",
      " [ 0.71179026]\n",
      " [ 0.7857162 ]\n",
      " [ 0.45931476]\n",
      " [ 0.25947022]\n",
      " [ 0.31396455]\n",
      " [ 1.0049875 ]\n",
      " [ 0.730857  ]\n",
      " [ 0.68420553]\n",
      " [ 0.5472954 ]\n",
      " [ 0.3846609 ]\n",
      " [ 0.16197515]\n",
      " [ 0.6863475 ]\n",
      " [ 0.3685792 ]\n",
      " [ 0.20912917]\n",
      " [ 0.9982734 ]\n",
      " [ 0.31526437]\n",
      " [ 0.3522734 ]\n",
      " [ 0.78208435]\n",
      " [ 0.4455399 ]\n",
      " [ 0.64573467]\n",
      " [ 0.58862644]\n",
      " [ 0.606141  ]\n",
      " [ 0.5874427 ]\n",
      " [ 0.9475829 ]\n",
      " [ 0.7312658 ]\n",
      " [ 0.40934074]\n",
      " [ 0.39795566]\n",
      " [ 0.6498385 ]\n",
      " [ 0.6057721 ]\n",
      " [ 0.1315511 ]\n",
      " [ 0.50139475]\n",
      " [ 0.7369387 ]\n",
      " [ 0.8024918 ]\n",
      " [ 0.41788936]\n",
      " [ 0.8545075 ]\n",
      " [ 0.19422889]\n",
      " [ 0.6587527 ]\n",
      " [ 0.22248286]\n",
      " [ 0.5058218 ]\n",
      " [ 0.38619775]\n",
      " [ 0.77827424]\n",
      " [ 0.56655514]\n",
      " [ 0.5958971 ]\n",
      " [ 0.2454374 ]\n",
      " [ 0.48475096]\n",
      " [ 0.911823  ]\n",
      " [ 0.6149571 ]\n",
      " [ 0.5659964 ]\n",
      " [ 0.2451931 ]\n",
      " [ 0.3968158 ]\n",
      " [ 1.1759361 ]\n",
      " [ 0.7352556 ]\n",
      " [ 0.22932647]\n",
      " [ 0.43852288]\n",
      " [ 0.6675567 ]\n",
      " [ 0.6369673 ]\n",
      " [ 0.2789659 ]\n",
      " [ 0.54128194]\n",
      " [ 1.0253427 ]\n",
      " [ 0.83055854]\n",
      " [ 0.6436856 ]\n",
      " [ 0.5355549 ]\n",
      " [ 0.5098827 ]\n",
      " [ 0.9766953 ]\n",
      " [ 0.6013658 ]\n",
      " [ 0.60453546]\n",
      " [ 1.5499275 ]\n",
      " [ 0.43736687]\n",
      " [ 0.9567393 ]\n",
      " [ 0.456398  ]\n",
      " [ 0.68632257]\n",
      " [ 0.7680792 ]\n",
      " [ 0.79426146]\n",
      " [ 0.93580234]\n",
      " [ 0.72667545]\n",
      " [ 0.42277068]\n",
      " [ 0.9957322 ]\n",
      " [ 0.4057501 ]\n",
      " [ 0.7820053 ]\n",
      " [ 0.27637294]\n",
      " [ 0.2675386 ]\n",
      " [ 0.76255023]\n",
      " [ 0.9078187 ]\n",
      " [ 0.79389745]\n",
      " [ 0.5862782 ]\n",
      " [ 0.9288137 ]\n",
      " [ 0.79861623]\n",
      " [ 0.5715419 ]\n",
      " [ 0.79853046]\n",
      " [ 0.36469147]\n",
      " [ 0.35993883]\n",
      " [ 0.66142035]\n",
      " [ 0.63645995]\n",
      " [ 0.7768641 ]\n",
      " [ 0.34857264]\n",
      " [ 0.52978456]\n",
      " [ 0.06820999]\n",
      " [ 0.84300447]\n",
      " [ 0.7234447 ]\n",
      " [ 0.7861463 ]\n",
      " [ 0.7030196 ]\n",
      " [ 0.618569  ]\n",
      " [ 0.84263515]\n",
      " [ 0.75865537]\n",
      " [ 1.0716648 ]\n",
      " [ 0.5053275 ]\n",
      " [ 0.31894958]\n",
      " [ 0.55334044]\n",
      " [ 0.34341922]\n",
      " [ 0.35279715]\n",
      " [ 0.38515863]\n",
      " [ 0.45017964]\n",
      " [ 0.47693104]\n",
      " [ 0.5729266 ]\n",
      " [ 0.4793472 ]\n",
      " [ 0.15328494]\n",
      " [ 0.92570674]\n",
      " [ 0.7055106 ]\n",
      " [ 0.5834363 ]\n",
      " [ 0.51088804]\n",
      " [ 0.28719112]\n",
      " [ 0.2685297 ]\n",
      " [ 0.66055465]\n",
      " [ 0.92468655]\n",
      " [ 0.2591106 ]\n",
      " [ 0.6307738 ]\n",
      " [ 0.71852684]\n",
      " [ 0.33211613]\n",
      " [ 0.47016287]\n",
      " [ 0.4160894 ]\n",
      " [ 0.49607345]\n",
      " [ 0.6850302 ]\n",
      " [ 0.53285146]\n",
      " [ 0.6683289 ]\n",
      " [ 0.8717215 ]\n",
      " [ 0.7569896 ]\n",
      " [ 0.6032433 ]\n",
      " [ 0.5354024 ]\n",
      " [ 0.65732735]\n",
      " [ 0.4786254 ]\n",
      " [ 0.7538963 ]\n",
      " [ 0.47009623]\n",
      " [ 0.74850094]\n",
      " [ 0.7208085 ]\n",
      " [ 0.36942732]\n",
      " [ 0.42665124]\n",
      " [ 0.39715785]\n",
      " [ 0.64379966]\n",
      " [ 0.03044907]\n",
      " [ 0.39052778]\n",
      " [ 0.7840762 ]\n",
      " [ 0.12242893]\n",
      " [ 0.4430232 ]\n",
      " [ 0.4553791 ]\n",
      " [ 0.45164013]\n",
      " [ 0.26086232]\n",
      " [ 0.5294528 ]\n",
      " [ 0.55730456]\n",
      " [ 0.29262185]\n",
      " [ 0.42489707]\n",
      " [ 0.504708  ]\n",
      " [ 1.0516291 ]\n",
      " [ 0.5466205 ]\n",
      " [ 0.57181716]\n",
      " [ 0.87394065]\n",
      " [ 0.27010357]\n",
      " [ 0.77959085]\n",
      " [ 1.1025524 ]\n",
      " [ 0.79971576]\n",
      " [ 0.33165708]\n",
      " [ 0.3503581 ]\n",
      " [ 0.7907542 ]\n",
      " [ 1.0730376 ]\n",
      " [ 0.36676311]\n",
      " [ 0.54368865]\n",
      " [ 1.1315604 ]\n",
      " [ 0.37527096]\n",
      " [ 1.1893033 ]\n",
      " [ 0.713351  ]\n",
      " [ 0.40663368]\n",
      " [ 0.8593206 ]\n",
      " [ 0.514108  ]\n",
      " [ 0.96780276]\n",
      " [ 0.31621957]\n",
      " [ 0.31831223]\n",
      " [ 0.93120044]\n",
      " [ 0.7043184 ]\n",
      " [ 0.689682  ]\n",
      " [ 0.905504  ]\n",
      " [ 0.69999415]\n",
      " [ 0.45353663]\n",
      " [ 0.40151003]\n",
      " [ 0.6387735 ]\n",
      " [ 0.3623749 ]\n",
      " [ 0.6281972 ]\n",
      " [ 0.4717103 ]\n",
      " [ 0.5682125 ]\n",
      " [ 1.1059453 ]\n",
      " [ 0.4975836 ]\n",
      " [ 0.58684903]\n",
      " [ 0.7184764 ]\n",
      " [ 0.21401435]\n",
      " [ 0.49431723]\n",
      " [ 0.33160362]\n",
      " [ 0.2574934 ]\n",
      " [ 0.49579006]\n",
      " [ 0.9263384 ]\n",
      " [ 0.3802765 ]\n",
      " [ 0.3997166 ]\n",
      " [ 0.5295987 ]\n",
      " [ 0.72367424]\n",
      " [ 0.44486684]\n",
      " [ 1.1937336 ]\n",
      " [ 0.5796626 ]\n",
      " [ 0.3489688 ]\n",
      " [ 0.32293636]\n",
      " [ 0.34641033]\n",
      " [ 0.7450093 ]\n",
      " [ 0.7883638 ]\n",
      " [ 0.14435655]\n",
      " [ 0.46911693]\n",
      " [ 0.3003228 ]\n",
      " [ 0.7727084 ]\n",
      " [ 0.6449011 ]\n",
      " [ 0.6382952 ]\n",
      " [ 0.8491723 ]\n",
      " [ 0.61963445]\n",
      " [ 0.29531687]\n",
      " [ 0.80252784]\n",
      " [ 1.048473  ]\n",
      " [ 0.7818115 ]\n",
      " [ 0.4756599 ]\n",
      " [ 0.30339324]\n",
      " [ 0.52874315]\n",
      " [ 0.23075524]\n",
      " [ 0.5330837 ]\n",
      " [ 0.52381533]\n",
      " [ 0.6611484 ]\n",
      " [ 0.8032076 ]\n",
      " [ 0.5988505 ]\n",
      " [ 0.81640667]\n",
      " [ 0.6871293 ]\n",
      " [ 0.40972337]\n",
      " [ 0.5788802 ]\n",
      " [ 0.6517081 ]\n",
      " [ 0.585868  ]\n",
      " [ 0.37054658]\n",
      " [ 0.6186581 ]\n",
      " [ 0.15252008]\n",
      " [ 0.74411356]\n",
      " [ 0.9891281 ]\n",
      " [ 0.41138902]\n",
      " [ 0.72160137]\n",
      " [ 1.2185014 ]\n",
      " [ 0.30944288]\n",
      " [ 0.42812818]\n",
      " [ 0.5037604 ]\n",
      " [ 0.18975791]\n",
      " [ 0.66068816]\n",
      " [ 0.8675178 ]\n",
      " [ 1.0305295 ]\n",
      " [ 0.42705825]\n",
      " [ 0.5017176 ]\n",
      " [ 0.5507409 ]\n",
      " [ 0.7790664 ]\n",
      " [ 0.14770553]\n",
      " [ 0.95926785]\n",
      " [ 0.82235175]\n",
      " [ 0.7240464 ]\n",
      " [ 0.5674194 ]\n",
      " [-0.13490328]\n",
      " [ 0.9125923 ]\n",
      " [ 0.42857552]\n",
      " [ 0.87913156]\n",
      " [ 0.5373859 ]\n",
      " [ 0.6610406 ]\n",
      " [ 0.30008686]\n",
      " [ 0.53393215]\n",
      " [ 0.7691072 ]\n",
      " [ 0.59433603]\n",
      " [ 0.6579065 ]\n",
      " [ 0.47688478]\n",
      " [ 0.04867717]\n",
      " [ 0.61192656]\n",
      " [ 0.6127614 ]\n",
      " [ 0.89845264]\n",
      " [ 0.7211039 ]\n",
      " [ 1.252128  ]\n",
      " [ 0.5565364 ]\n",
      " [ 0.47876492]\n",
      " [ 0.38180825]\n",
      " [ 0.631578  ]\n",
      " [ 0.71122897]\n",
      " [ 0.5311812 ]\n",
      " [ 1.0022852 ]\n",
      " [ 0.45687714]\n",
      " [ 0.8849608 ]]\n"
     ]
    }
   ],
   "source": [
    "codalab_data = pd.read_csv('public_dev.csv')\n",
    "test_data_codalab = codalab_data['text'].values\n",
    "results_codalab = model.predict(test_data_codalab)\n",
    "print(results_codalab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71463233\n",
      "0.466227\n",
      "0.43464866\n",
      "0.5293789\n",
      "0.6286638\n",
      "0.30118436\n",
      "0.6193511\n",
      "0.95704293\n",
      "0.7478211\n",
      "0.43221325\n",
      "0.83511585\n",
      "0.5363073\n",
      "0.4228049\n",
      "0.86665577\n",
      "0.43635595\n",
      "0.17332546\n",
      "0.53414094\n",
      "0.45308346\n",
      "0.6421553\n",
      "0.020272987\n",
      "0.38690934\n",
      "0.26311034\n",
      "0.87128294\n",
      "0.33200538\n",
      "0.5597373\n",
      "0.555234\n",
      "0.5751617\n",
      "0.5611706\n",
      "0.5191389\n",
      "0.44830987\n",
      "0.37261343\n",
      "0.58720833\n",
      "0.5899044\n",
      "0.7572112\n",
      "0.29644203\n",
      "1.0250517\n",
      "0.5845968\n",
      "0.33077806\n",
      "0.67161405\n",
      "0.5704586\n",
      "0.42673382\n",
      "0.4787372\n",
      "0.5068246\n",
      "0.2629834\n",
      "0.1727949\n",
      "0.7206178\n",
      "0.33040145\n",
      "0.51743907\n",
      "0.2683318\n",
      "0.70921516\n",
      "0.8130994\n",
      "0.40617532\n",
      "0.32237297\n",
      "0.51842517\n",
      "0.5116469\n",
      "0.5600267\n",
      "0.51031935\n",
      "0.46783584\n",
      "0.9689901\n",
      "0.5859816\n",
      "0.3487292\n",
      "0.23320568\n",
      "0.6203511\n",
      "0.7501102\n",
      "0.6104374\n",
      "0.80066204\n",
      "0.25312746\n",
      "0.37512097\n",
      "0.12652475\n",
      "0.7649709\n",
      "0.4499385\n",
      "0.916028\n",
      "0.61808854\n",
      "0.97077876\n",
      "1.5198821\n",
      "0.48097432\n",
      "0.7572388\n",
      "0.32888997\n",
      "0.53183204\n",
      "0.30134413\n",
      "0.524349\n",
      "0.5404148\n",
      "0.84546685\n",
      "0.26987785\n",
      "0.38601333\n",
      "0.23680799\n",
      "0.44103825\n",
      "0.7507548\n",
      "1.1322055\n",
      "0.86374617\n",
      "0.7391016\n",
      "0.90799403\n",
      "0.7332568\n",
      "0.5814266\n",
      "0.62877566\n",
      "0.14471418\n",
      "0.1987424\n",
      "0.4166115\n",
      "0.54519004\n",
      "0.15739153\n",
      "0.23567821\n",
      "0.3051163\n",
      "0.5254662\n",
      "0.8506402\n",
      "0.3983803\n",
      "0.2800739\n",
      "0.9515102\n",
      "0.35966343\n",
      "0.6455914\n",
      "0.6254227\n",
      "0.7776499\n",
      "0.9270312\n",
      "0.13838635\n",
      "0.6566226\n",
      "0.29319015\n",
      "0.021275694\n",
      "0.9488485\n",
      "0.8320296\n",
      "0.1469797\n",
      "0.44155568\n",
      "0.5337797\n",
      "0.89657426\n",
      "0.20951624\n",
      "0.7177329\n",
      "0.7428999\n",
      "0.2135152\n",
      "0.5754504\n",
      "0.54062855\n",
      "1.1220545\n",
      "0.7110819\n",
      "1.0435146\n",
      "0.8218125\n",
      "0.65954745\n",
      "0.6108628\n",
      "0.30684304\n",
      "0.60936546\n",
      "0.17836533\n",
      "0.42879552\n",
      "0.3426816\n",
      "0.6422542\n",
      "0.6534327\n",
      "0.15204497\n",
      "0.4321615\n",
      "0.29727703\n",
      "0.57410425\n",
      "0.12138163\n",
      "0.21326558\n",
      "0.16330475\n",
      "0.1494412\n",
      "0.84136105\n",
      "0.6560687\n",
      "0.52678645\n",
      "0.49395758\n",
      "1.1510329\n",
      "0.616276\n",
      "0.7985661\n",
      "0.6621012\n",
      "0.7216016\n",
      "0.5251738\n",
      "0.33139592\n",
      "0.76286685\n",
      "0.3173465\n",
      "0.09513809\n",
      "0.36908048\n",
      "0.6733763\n",
      "0.28939775\n",
      "0.33070624\n",
      "0.2534113\n",
      "0.61706144\n",
      "0.8167056\n",
      "0.43249935\n",
      "0.5374416\n",
      "1.2171848\n",
      "0.35395747\n",
      "1.0490432\n",
      "1.0835631\n",
      "0.5682747\n",
      "0.6441429\n",
      "0.79343325\n",
      "0.85054874\n",
      "0.9446893\n",
      "0.4760629\n",
      "0.10167602\n",
      "0.69062245\n",
      "0.7353049\n",
      "0.40291592\n",
      "0.40895534\n",
      "0.6125211\n",
      "0.48917526\n",
      "0.59355736\n",
      "0.49948785\n",
      "0.589579\n",
      "0.8145673\n",
      "0.6894296\n",
      "0.53853065\n",
      "0.75623095\n",
      "0.56479406\n",
      "0.3095237\n",
      "0.45154762\n",
      "0.73904014\n",
      "0.440409\n",
      "0.34989694\n",
      "0.52813184\n",
      "0.33167416\n",
      "0.6667253\n",
      "0.51356524\n",
      "0.73631155\n",
      "0.69964105\n",
      "0.571654\n",
      "1.1243953\n",
      "0.83539116\n",
      "0.31400663\n",
      "0.33647576\n",
      "0.5859485\n",
      "0.8094928\n",
      "0.6260703\n",
      "0.67228866\n",
      "0.4600282\n",
      "0.7700167\n",
      "0.74882066\n",
      "0.4123844\n",
      "0.3095146\n",
      "0.48003352\n",
      "0.26961744\n",
      "0.40723878\n",
      "0.3121245\n",
      "0.18517835\n",
      "0.7754424\n",
      "0.76322734\n",
      "0.51333845\n",
      "0.7591017\n",
      "0.71300983\n",
      "0.39473665\n",
      "0.6577942\n",
      "0.6037127\n",
      "0.42535502\n",
      "0.36529177\n",
      "0.84063435\n",
      "0.11438248\n",
      "0.5800841\n",
      "0.6898317\n",
      "0.3516814\n",
      "0.60283643\n",
      "0.59555006\n",
      "0.4545334\n",
      "0.49371582\n",
      "0.9621471\n",
      "0.91416526\n",
      "0.54182774\n",
      "0.70103323\n",
      "0.32077768\n",
      "0.4735967\n",
      "0.41383952\n",
      "0.73881334\n",
      "0.58467937\n",
      "0.8850627\n",
      "0.56177294\n",
      "0.63264334\n",
      "0.44002506\n",
      "0.5634033\n",
      "0.5592455\n",
      "0.78148156\n",
      "0.60581756\n",
      "0.9076333\n",
      "0.7442875\n",
      "0.5039125\n",
      "0.30675456\n",
      "0.20442326\n",
      "0.81314015\n",
      "0.48114726\n",
      "0.763252\n",
      "0.8362066\n",
      "0.43177497\n",
      "0.25129497\n",
      "0.4813041\n",
      "0.35262883\n",
      "0.3423215\n",
      "1.0228114\n",
      "0.33007646\n",
      "0.3636307\n",
      "0.4767476\n",
      "0.52222455\n",
      "0.27909768\n",
      "1.3191458\n",
      "0.96173143\n",
      "0.3846217\n",
      "0.39417636\n",
      "0.46931708\n",
      "0.9243719\n",
      "0.78820133\n",
      "0.4999315\n",
      "0.7965777\n",
      "0.87261677\n",
      "0.96003044\n",
      "0.33941868\n",
      "0.59588414\n",
      "0.17129181\n",
      "0.65848964\n",
      "0.7148702\n",
      "0.6780851\n",
      "0.52373385\n",
      "0.50637084\n",
      "0.7503251\n",
      "0.55864346\n",
      "0.83656377\n",
      "0.6218963\n",
      "1.2040802\n",
      "0.48529273\n",
      "0.87173223\n",
      "0.66044295\n",
      "0.8966657\n",
      "0.6824763\n",
      "0.76811373\n",
      "0.41534984\n",
      "0.6448955\n",
      "0.6263097\n",
      "0.35094395\n",
      "0.8414421\n",
      "0.5738741\n",
      "0.38990614\n",
      "0.4416047\n",
      "0.29944494\n",
      "0.5286689\n",
      "0.06672755\n",
      "0.57404244\n",
      "0.3098408\n",
      "0.63023317\n",
      "0.3447076\n",
      "1.0016748\n",
      "0.6349784\n",
      "0.18561211\n",
      "1.0763549\n",
      "0.2669721\n",
      "0.51141864\n",
      "0.4560756\n",
      "0.16296738\n",
      "0.8894282\n",
      "0.6382797\n",
      "0.43324468\n",
      "0.23653646\n",
      "1.0466522\n",
      "1.0134385\n",
      "0.7136344\n",
      "0.6654942\n",
      "0.70210886\n",
      "0.12286934\n",
      "0.705279\n",
      "0.20647559\n",
      "0.5974566\n",
      "0.46022695\n",
      "0.53578544\n",
      "0.44197702\n",
      "0.7406144\n",
      "1.1552165\n",
      "0.59599245\n",
      "0.3557064\n",
      "0.44132903\n",
      "0.7296624\n",
      "0.49179912\n",
      "0.17013788\n",
      "0.7138654\n",
      "0.74644196\n",
      "0.39748248\n",
      "0.48367468\n",
      "0.45369703\n",
      "0.26764056\n",
      "0.47516683\n",
      "0.43131402\n",
      "0.3215995\n",
      "0.8038509\n",
      "0.3989661\n",
      "1.084079\n",
      "0.1366159\n",
      "0.098254755\n",
      "0.4861251\n",
      "0.07312544\n",
      "0.69569683\n",
      "0.36152503\n",
      "0.54978204\n",
      "0.4519971\n",
      "0.46468967\n",
      "0.73478484\n",
      "0.51369566\n",
      "0.6189972\n",
      "0.6588748\n",
      "0.42300928\n",
      "0.50988644\n",
      "0.6314018\n",
      "0.50001913\n",
      "0.5475148\n",
      "0.42429972\n",
      "0.3926274\n",
      "0.41293085\n",
      "0.3209427\n",
      "0.51494944\n",
      "0.38840866\n",
      "0.60024714\n",
      "0.564743\n",
      "0.6157357\n",
      "0.4554803\n",
      "1.1705469\n",
      "0.9857444\n",
      "0.6001477\n",
      "0.9301555\n",
      "0.6753113\n",
      "0.721929\n",
      "0.363455\n",
      "0.49052626\n",
      "0.5206347\n",
      "1.0383259\n",
      "0.55214965\n",
      "0.46520966\n",
      "0.9462118\n",
      "0.72242546\n",
      "0.6294786\n",
      "0.9047749\n",
      "0.37588084\n",
      "0.7460058\n",
      "0.7481633\n",
      "0.81068426\n",
      "0.67539215\n",
      "0.5931819\n",
      "0.94057614\n",
      "0.36627263\n",
      "0.47442\n",
      "0.84853816\n",
      "0.3118994\n",
      "0.32165295\n",
      "0.50490135\n",
      "0.48424482\n",
      "0.8053835\n",
      "0.73330474\n",
      "0.396422\n",
      "0.34896123\n",
      "1.1056591\n",
      "0.39477214\n",
      "0.72766984\n",
      "0.5135357\n",
      "0.56161535\n",
      "0.44942108\n",
      "0.44511417\n",
      "0.7594204\n",
      "0.6401163\n",
      "0.6300001\n",
      "0.8022307\n",
      "0.7954041\n",
      "0.4173813\n",
      "0.34009996\n",
      "0.6674798\n",
      "0.55452645\n",
      "0.56677276\n",
      "0.29671866\n",
      "0.69263506\n",
      "0.65417534\n",
      "0.32096967\n",
      "0.40098026\n",
      "0.55857325\n",
      "0.7307818\n",
      "0.31953448\n",
      "0.44078127\n",
      "0.5436281\n",
      "0.6397779\n",
      "0.4726931\n",
      "0.47173333\n",
      "0.29043508\n",
      "0.2989992\n",
      "0.4700541\n",
      "0.6237142\n",
      "0.629729\n",
      "0.7008346\n",
      "0.6529277\n",
      "0.47691375\n",
      "0.73861206\n",
      "0.9825152\n",
      "0.76560974\n",
      "0.38717145\n",
      "0.54245174\n",
      "0.6220983\n",
      "0.48271844\n",
      "0.60584295\n",
      "0.52565855\n",
      "0.43326807\n",
      "0.42888337\n",
      "0.4413181\n",
      "0.62865424\n",
      "0.67871857\n",
      "0.6284976\n",
      "0.67835164\n",
      "0.411901\n",
      "0.52531487\n",
      "0.7308283\n",
      "0.5942439\n",
      "0.6117791\n",
      "1.1926433\n",
      "0.89440364\n",
      "0.5943496\n",
      "0.642686\n",
      "0.5074145\n",
      "0.71475714\n",
      "0.7945845\n",
      "0.49727756\n",
      "0.60023487\n",
      "1.107748\n",
      "0.9329715\n",
      "0.7203267\n",
      "0.61384463\n",
      "1.0364817\n",
      "0.7108178\n",
      "0.40020972\n",
      "0.49838245\n",
      "0.6891608\n",
      "0.56119806\n",
      "0.9602291\n",
      "0.87931174\n",
      "0.6858274\n",
      "0.39186192\n",
      "0.5672486\n",
      "0.45877373\n",
      "0.772926\n",
      "0.813704\n",
      "0.6222287\n",
      "0.33957613\n",
      "0.83798623\n",
      "0.45978987\n",
      "0.5445262\n",
      "0.82515377\n",
      "0.79594356\n",
      "0.47290412\n",
      "0.42603728\n",
      "0.54737854\n",
      "0.5652505\n",
      "0.5819737\n",
      "0.38303357\n",
      "0.65522265\n",
      "1.1323677\n",
      "0.8037579\n",
      "0.36190963\n",
      "0.92452514\n",
      "0.6899696\n",
      "0.26324075\n",
      "0.5732256\n",
      "0.3734098\n",
      "0.5361466\n",
      "0.55699503\n",
      "0.28758729\n",
      "0.5460557\n",
      "1.075265\n",
      "0.6788966\n",
      "0.4079473\n",
      "0.6190171\n",
      "0.4108885\n",
      "0.9289551\n",
      "1.0531987\n",
      "0.68437386\n",
      "0.81355524\n",
      "0.80170643\n",
      "0.6400967\n",
      "0.41938668\n",
      "0.30899853\n",
      "0.2766247\n",
      "0.28098977\n",
      "0.10635577\n",
      "0.6526289\n",
      "0.6075189\n",
      "0.3655459\n",
      "0.33116773\n",
      "0.9236419\n",
      "0.73558056\n",
      "0.38659438\n",
      "0.45650524\n",
      "0.7931905\n",
      "0.3875561\n",
      "0.61741287\n",
      "0.6136075\n",
      "0.5393326\n",
      "0.3246374\n",
      "0.5079174\n",
      "0.04342503\n",
      "0.5067402\n",
      "0.29120225\n",
      "0.5364494\n",
      "0.9196981\n",
      "0.34382468\n",
      "0.59600306\n",
      "0.23020999\n",
      "0.14227779\n",
      "1.0009296\n",
      "0.22734503\n",
      "0.14048646\n",
      "0.6057465\n",
      "0.37345645\n",
      "1.4860815\n",
      "0.80740565\n",
      "0.64127487\n",
      "0.65240824\n",
      "0.34698814\n",
      "0.52041006\n",
      "0.58443636\n",
      "0.54021066\n",
      "0.68417513\n",
      "0.47103775\n",
      "0.2672689\n",
      "0.8494611\n",
      "0.56959295\n",
      "0.56107044\n",
      "0.43598023\n",
      "0.71190846\n",
      "0.26780394\n",
      "0.4767317\n",
      "0.36777133\n",
      "0.378623\n",
      "0.53548217\n",
      "0.46080983\n",
      "0.77230906\n",
      "0.45812905\n",
      "0.9083637\n",
      "0.5324098\n",
      "1.0064106\n",
      "0.72222584\n",
      "0.35855928\n",
      "0.72243804\n",
      "0.82443476\n",
      "0.5840508\n",
      "0.97227764\n",
      "0.3114922\n",
      "0.5284109\n",
      "0.8063226\n",
      "0.7466629\n",
      "0.57028604\n",
      "0.21187063\n",
      "0.89199054\n",
      "0.8905626\n",
      "0.3095851\n",
      "0.2703718\n",
      "0.6696203\n",
      "0.3902613\n",
      "0.55478275\n",
      "0.21522231\n",
      "0.18687068\n",
      "0.8256113\n",
      "0.8164475\n",
      "0.8439425\n",
      "1.0320001\n",
      "0.47059944\n",
      "0.5759507\n",
      "0.7038803\n",
      "0.43396318\n",
      "0.43488947\n",
      "0.9817515\n",
      "0.34563863\n",
      "0.45212302\n",
      "0.28733227\n",
      "0.66701806\n",
      "0.506821\n",
      "0.8051656\n",
      "0.740536\n",
      "0.77988446\n",
      "0.7224845\n",
      "0.66080475\n",
      "0.6143031\n",
      "0.6845118\n",
      "0.47331372\n",
      "0.65870893\n",
      "0.8877018\n",
      "0.90261626\n",
      "0.5831131\n",
      "0.43463534\n",
      "0.5434541\n",
      "0.39878836\n",
      "0.43821514\n",
      "0.87951964\n",
      "0.6546079\n",
      "0.31155267\n",
      "0.535766\n",
      "0.6697166\n",
      "0.61786497\n",
      "0.64739007\n",
      "0.67334074\n",
      "0.4728004\n",
      "0.8706361\n",
      "0.8132169\n",
      "0.45925233\n",
      "0.670107\n",
      "0.7214527\n",
      "0.5835343\n",
      "0.7125502\n",
      "0.46990645\n",
      "0.82062465\n",
      "0.7570512\n",
      "0.5732835\n",
      "0.292735\n",
      "0.35046002\n",
      "0.6873214\n",
      "0.5525069\n",
      "0.72843015\n",
      "0.5982567\n",
      "0.5287448\n",
      "0.46965063\n",
      "0.6104019\n",
      "0.34206134\n",
      "0.8271035\n",
      "1.2724888\n",
      "0.61311054\n",
      "0.71179026\n",
      "0.7857162\n",
      "0.45931476\n",
      "0.25947022\n",
      "0.31396455\n",
      "1.0049875\n",
      "0.730857\n",
      "0.68420553\n",
      "0.5472954\n",
      "0.3846609\n",
      "0.16197515\n",
      "0.6863475\n",
      "0.3685792\n",
      "0.20912917\n",
      "0.9982734\n",
      "0.31526437\n",
      "0.3522734\n",
      "0.78208435\n",
      "0.4455399\n",
      "0.64573467\n",
      "0.58862644\n",
      "0.606141\n",
      "0.5874427\n",
      "0.9475829\n",
      "0.7312658\n",
      "0.40934074\n",
      "0.39795566\n",
      "0.6498385\n",
      "0.6057721\n",
      "0.1315511\n",
      "0.50139475\n",
      "0.7369387\n",
      "0.8024918\n",
      "0.41788936\n",
      "0.8545075\n",
      "0.19422889\n",
      "0.6587527\n",
      "0.22248286\n",
      "0.5058218\n",
      "0.38619775\n",
      "0.77827424\n",
      "0.56655514\n",
      "0.5958971\n",
      "0.2454374\n",
      "0.48475096\n",
      "0.911823\n",
      "0.6149571\n",
      "0.5659964\n",
      "0.2451931\n",
      "0.3968158\n",
      "1.1759361\n",
      "0.7352556\n",
      "0.22932647\n",
      "0.43852288\n",
      "0.6675567\n",
      "0.6369673\n",
      "0.2789659\n",
      "0.54128194\n",
      "1.0253427\n",
      "0.83055854\n",
      "0.6436856\n",
      "0.5355549\n",
      "0.5098827\n",
      "0.9766953\n",
      "0.6013658\n",
      "0.60453546\n",
      "1.5499275\n",
      "0.43736687\n",
      "0.9567393\n",
      "0.456398\n",
      "0.68632257\n",
      "0.7680792\n",
      "0.79426146\n",
      "0.93580234\n",
      "0.72667545\n",
      "0.42277068\n",
      "0.9957322\n",
      "0.4057501\n",
      "0.7820053\n",
      "0.27637294\n",
      "0.2675386\n",
      "0.76255023\n",
      "0.9078187\n",
      "0.79389745\n",
      "0.5862782\n",
      "0.9288137\n",
      "0.79861623\n",
      "0.5715419\n",
      "0.79853046\n",
      "0.36469147\n",
      "0.35993883\n",
      "0.66142035\n",
      "0.63645995\n",
      "0.7768641\n",
      "0.34857264\n",
      "0.52978456\n",
      "0.06820999\n",
      "0.84300447\n",
      "0.7234447\n",
      "0.7861463\n",
      "0.7030196\n",
      "0.618569\n",
      "0.84263515\n",
      "0.75865537\n",
      "1.0716648\n",
      "0.5053275\n",
      "0.31894958\n",
      "0.55334044\n",
      "0.34341922\n",
      "0.35279715\n",
      "0.38515863\n",
      "0.45017964\n",
      "0.47693104\n",
      "0.5729266\n",
      "0.4793472\n",
      "0.15328494\n",
      "0.92570674\n",
      "0.7055106\n",
      "0.5834363\n",
      "0.51088804\n",
      "0.28719112\n",
      "0.2685297\n",
      "0.66055465\n",
      "0.92468655\n",
      "0.2591106\n",
      "0.6307738\n",
      "0.71852684\n",
      "0.33211613\n",
      "0.47016287\n",
      "0.4160894\n",
      "0.49607345\n",
      "0.6850302\n",
      "0.53285146\n",
      "0.6683289\n",
      "0.8717215\n",
      "0.7569896\n",
      "0.6032433\n",
      "0.5354024\n",
      "0.65732735\n",
      "0.4786254\n",
      "0.7538963\n",
      "0.47009623\n",
      "0.74850094\n",
      "0.7208085\n",
      "0.36942732\n",
      "0.42665124\n",
      "0.39715785\n",
      "0.64379966\n",
      "0.030449072\n",
      "0.39052778\n",
      "0.7840762\n",
      "0.12242893\n",
      "0.4430232\n",
      "0.4553791\n",
      "0.45164013\n",
      "0.26086232\n",
      "0.5294528\n",
      "0.55730456\n",
      "0.29262185\n",
      "0.42489707\n",
      "0.504708\n",
      "1.0516291\n",
      "0.5466205\n",
      "0.57181716\n",
      "0.87394065\n",
      "0.27010357\n",
      "0.77959085\n",
      "1.1025524\n",
      "0.79971576\n",
      "0.33165708\n",
      "0.3503581\n",
      "0.7907542\n",
      "1.0730376\n",
      "0.36676311\n",
      "0.54368865\n",
      "1.1315604\n",
      "0.37527096\n",
      "1.1893033\n",
      "0.713351\n",
      "0.40663368\n",
      "0.8593206\n",
      "0.514108\n",
      "0.96780276\n",
      "0.31621957\n",
      "0.31831223\n",
      "0.93120044\n",
      "0.7043184\n",
      "0.689682\n",
      "0.905504\n",
      "0.69999415\n",
      "0.45353663\n",
      "0.40151003\n",
      "0.6387735\n",
      "0.3623749\n",
      "0.6281972\n",
      "0.4717103\n",
      "0.5682125\n",
      "1.1059453\n",
      "0.4975836\n",
      "0.58684903\n",
      "0.7184764\n",
      "0.21401435\n",
      "0.49431723\n",
      "0.33160362\n",
      "0.2574934\n",
      "0.49579006\n",
      "0.9263384\n",
      "0.3802765\n",
      "0.3997166\n",
      "0.5295987\n",
      "0.72367424\n",
      "0.44486684\n",
      "1.1937336\n",
      "0.5796626\n",
      "0.3489688\n",
      "0.32293636\n",
      "0.34641033\n",
      "0.7450093\n",
      "0.7883638\n",
      "0.14435655\n",
      "0.46911693\n",
      "0.3003228\n",
      "0.7727084\n",
      "0.6449011\n",
      "0.6382952\n",
      "0.8491723\n",
      "0.61963445\n",
      "0.29531687\n",
      "0.80252784\n",
      "1.048473\n",
      "0.7818115\n",
      "0.4756599\n",
      "0.30339324\n",
      "0.52874315\n",
      "0.23075524\n",
      "0.5330837\n",
      "0.52381533\n",
      "0.6611484\n",
      "0.8032076\n",
      "0.5988505\n",
      "0.81640667\n",
      "0.6871293\n",
      "0.40972337\n",
      "0.5788802\n",
      "0.6517081\n",
      "0.585868\n",
      "0.37054658\n",
      "0.6186581\n",
      "0.15252008\n",
      "0.74411356\n",
      "0.9891281\n",
      "0.41138902\n",
      "0.72160137\n",
      "1.2185014\n",
      "0.30944288\n",
      "0.42812818\n",
      "0.5037604\n",
      "0.18975791\n",
      "0.66068816\n",
      "0.8675178\n",
      "1.0305295\n",
      "0.42705825\n",
      "0.5017176\n",
      "0.5507409\n",
      "0.7790664\n",
      "0.14770553\n",
      "0.95926785\n",
      "0.82235175\n",
      "0.7240464\n",
      "0.5674194\n",
      "-0.13490328\n",
      "0.9125923\n",
      "0.42857552\n",
      "0.87913156\n",
      "0.5373859\n",
      "0.6610406\n",
      "0.30008686\n",
      "0.53393215\n",
      "0.7691072\n",
      "0.59433603\n",
      "0.6579065\n",
      "0.47688478\n",
      "0.04867717\n",
      "0.61192656\n",
      "0.6127614\n",
      "0.89845264\n",
      "0.7211039\n",
      "1.252128\n",
      "0.5565364\n",
      "0.47876492\n",
      "0.38180825\n",
      "0.631578\n",
      "0.71122897\n",
      "0.5311812\n",
      "1.0022852\n",
      "0.45687714\n",
      "0.8849608\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(results_codalab)):\n",
    "    print(results_codalab[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
