{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, re, time\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                               text  is_humor  \\\n",
      "0   1  TENNESSEE: We're the best state. Nobody even c...         1   \n",
      "1   2  A man inserted an advertisement in the classif...         1   \n",
      "2   3  How many men does it take to open a can of bee...         1   \n",
      "3   4  Told my mom I hit 1200 Twitter followers. She ...         1   \n",
      "4   5  Roses are dead. Love is fake. Weddings are bas...         1   \n",
      "\n",
      "   humor_rating  humor_controversy  offense_rating  \n",
      "0          2.42                1.0             0.2  \n",
      "1          2.50                1.0             1.1  \n",
      "2          1.95                0.0             2.4  \n",
      "3          2.11                1.0             0.0  \n",
      "4          2.78                0.0             0.1  \n",
      "-------------------------------------------------------------------------\n",
      "id                   False\n",
      "text                 False\n",
      "is_humor             False\n",
      "humor_rating          True\n",
      "humor_controversy     True\n",
      "offense_rating       False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "#data = dataset[['text', 'is_humor']]\n",
    "\n",
    "print(data.head())\n",
    "print('-------------------------------------------------------------------------')\n",
    "print(data.isnull().any(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relacing special symbols and digits in headline column\n",
    "# re stands for Regular Expression\n",
    "data['text'] = data['text'].apply(lambda s : re.sub('[^a-zA-Z]', ' ', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                               text  is_humor  \\\n",
      "0   1  TENNESSEE  We re the best state  Nobody even c...         1   \n",
      "1   2  A man inserted an advertisement in the classif...         1   \n",
      "2   3  How many men does it take to open a can of bee...         1   \n",
      "3   4  Told my mom I hit      Twitter followers  She ...         1   \n",
      "4   5  Roses are dead  Love is fake  Weddings are bas...         1   \n",
      "\n",
      "   humor_rating  humor_controversy  offense_rating  \n",
      "0          2.42                1.0             0.2  \n",
      "1          2.50                1.0             1.1  \n",
      "2          1.95                0.0             2.4  \n",
      "3          2.11                1.0             0.0  \n",
      "4          2.78                0.0             0.1  \n",
      "-------------------------------------------------------------------------\n",
      "id                   False\n",
      "text                 False\n",
      "is_humor             False\n",
      "humor_rating          True\n",
      "humor_controversy     True\n",
      "offense_rating       False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(data.head())\n",
    "print('-------------------------------------------------------------------------')\n",
    "print(data.isnull().any(axis = 0))\n",
    "\n",
    "# getting features and labels\n",
    "features = data['text']\n",
    "labels = data['is_humor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       tennesse We re the best state nobodi even come...\n",
      "1       A man insert an advertis in the classifi wife ...\n",
      "2       how mani men doe it take to open a can of beer...\n",
      "3       told my mom I hit twitter follow she point out...\n",
      "4       rose are dead love is fake wed are basic funer...\n",
      "                              ...                        \n",
      "7995    lack of awar of the pervas of racism in our so...\n",
      "7996         whi are aspirin white becaus they work sorri\n",
      "7997    today we american celebr our independ from bri...\n",
      "7998    how to keep the fli off the bride at an italia...\n",
      "7999    each ounc of sunflow seed give you of your dai...\n",
      "Name: text, Length: 8000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Stemming our data\n",
    "ps = PorterStemmer()\n",
    "features = features.apply(lambda x: x.split())\n",
    "\n",
    "features = features.apply(lambda x : ' '.join([ps.stem(word) for word in x]))\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# vectorizing the data with maximum of 5000 features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer(max_features = 3223)\n",
    "features = list(features)\n",
    "features = tv.fit_transform(features).toarray()\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting training and testing data\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96328125\n",
      "0.851875\n"
     ]
    }
   ],
   "source": [
    "# model 1:-\n",
    "# Using linear support vector classifier\n",
    "lsvc = LinearSVC()\n",
    "# training the model\n",
    "lsvc.fit(features_train, labels_train)\n",
    "#getting the score of train and test data\n",
    "print(lsvc.score(features_train, labels_train)) # 90.93\n",
    "print(lsvc.score(features_test, labels_test))   # 83.75\n",
    "# model 2:-\n",
    "# Using Gaussuan Naive Bayes\n",
    "# gnb = GaussianNB()\n",
    "# gnb.fit(features_train, labels_train)\n",
    "# print(gnb.score(features_train, labels_train))  # 78.86\n",
    "# print(gnb.score(features_test, labels_test))    # 73.80\n",
    "# # model 3:-\n",
    "# # Logistic Regression\n",
    "# lr = LogisticRegression()\n",
    "# lr.fit(features_train, labels_train)\n",
    "# print(lr.score(features_train, labels_train))   # 88.16\n",
    "# print(lr.score(features_test, labels_test))     # 83.08\n",
    "# # model 4:-\n",
    "# # Random Forest Classifier\n",
    "#rfc = RandomForestClassifier(n_estimators = 10, random_state = 0)\n",
    "#rfc.fit(features_train, labels_train)\n",
    "#print(rfc.score(features_train, labels_train))  # 98.82\n",
    "#print(rfc.score(features_test, labels_test))    # 79.71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id                                               text  is_humor\n",
      "0    8001  What's the difference between a Bernie Sanders...         1\n",
      "1    8002     Vodka, whisky, tequila. I'm calling the shots.         0\n",
      "2    8003     French people don't masturbate They Jacque off         1\n",
      "3    8004  A lot of Suicide bombers are Muslims - I don't...         0\n",
      "4    8005  What happens when you fingerbang a gypsy on he...         0\n",
      "..    ...                                                ...       ...\n",
      "995  8996  boss: what are you doing inventor of the bagpi...         0\n",
      "996  8997  I told him his views were pretty extreme and i...         1\n",
      "997  8998  \"Mum, all the black kids call each other Nigga...         0\n",
      "998  8999  In honor of Fathers Day, I'm gonna bring you \"...         1\n",
      "999  9000  I don't know why Coca-Cola and Pepsi are fight...         1\n",
      "\n",
      "[1000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "eval_data = pd.read_csv('public_dev.csv')\n",
    "print(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      What s the difference between a Bernie Sanders...\n",
      "1         Vodka  whisky  tequila  I m calling the shots \n",
      "2         French people don t masturbate They Jacque off\n",
      "3      A lot of Suicide bombers are Muslims   I don t...\n",
      "4      What happens when you fingerbang a gypsy on he...\n",
      "                             ...                        \n",
      "995    boss  what are you doing inventor of the bagpi...\n",
      "996    I told him his views were pretty extreme and i...\n",
      "997     Mum  all the black kids call each other Nigga...\n",
      "998    In honor of Fathers Day  I m gonna bring you  ...\n",
      "999    I don t know why Coca Cola and Pepsi are fight...\n",
      "Name: text, Length: 1000, dtype: object\n",
      "------------------------------------------------------------------\n",
      "0      what s the differ between a berni sander suppo...\n",
      "1                 vodka whiski tequila I m call the shot\n",
      "2              french peopl don t masturb they jacqu off\n",
      "3      A lot of suicid bomber are muslim I don t blam...\n",
      "4      what happen when you fingerbang a gypsi on her...\n",
      "                             ...                        \n",
      "995    boss what are you do inventor of the bagpip i ...\n",
      "996    I told him hi view were pretti extrem and it w...\n",
      "997    mum all the black kid call each other nigga bu...\n",
      "998    In honor of father day I m gonna bring you dad...\n",
      "999    I don t know whi coca cola and pepsi are fight...\n",
      "Name: text, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "eval_df = eval_data['text'].apply(lambda s : re.sub('[^a-zA-Z]', ' ', s))\n",
    "print(eval_df)\n",
    "print(\"------------------------------------------------------------------\")\n",
    "eval_df_features = eval_df.apply(lambda x: x.split())\n",
    "\n",
    "eval_df_features = eval_df_features.apply(lambda x : ' '.join([ps.stem(word) for word in x]))\n",
    "print(eval_df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "eval_df_features = list(eval_df_features)\n",
    "tv1 = TfidfVectorizer(max_features = 3223)\n",
    "\n",
    "eval_df_features = tv1.fit_transform(eval_df_features).toarray()\n",
    "print(eval_df_features)\n",
    "print(len(eval_df_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_df_features=tv1.transform(eval_df_features)\n",
    "#classifier.predict(X_test)\n",
    "humor_predict = lsvc.predict(eval_df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1\n",
      " 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1\n",
      " 1 1 1 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1\n",
      " 1 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1\n",
      " 0 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1\n",
      " 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1\n",
      " 0 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0 1 1 1\n",
      " 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0\n",
      " 1 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 1 1 0 1 0\n",
      " 0 0 0 1 1 0 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1\n",
      " 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1\n",
      " 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1\n",
      " 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
      " 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1\n",
      " 1 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1\n",
      " 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1\n",
      " 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1\n",
      " 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1\n",
      " 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0\n",
      " 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 0 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
      " 1 0 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 1 1\n",
      " 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "print(humor_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
